{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog post 4. Are you talking fashion? Building a fashion classifier for Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation of this implementation can be found at: http://www.rosariomgomez.me/ <br><br>\n",
    "__Index:__<br>\n",
    "1. Collecting data<br>\n",
    "2. Vectorize tweets<br>\n",
    "3. Machine learning algorithms<br>\n",
    "4. Miscalssifications with Logistic Regression<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Building the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = p.read_csv('./sub_obj_data/train_new.csv', usecols=(['class', 'text'])).dropna()\n",
    "test  = p.read_csv('./sub_obj_data/test_ds.csv', usecols=(['class', 'text'])).dropna()\n",
    "train = train.reindex(np.random.permutation(train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4162 1784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "training_data, test_data, training_labels, test_labels = train_test_split(train['text'].values, train['class'].values, test_size=0.3, random_state=0) #70-30 split\n",
    "print len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973 2973\n"
     ]
    }
   ],
   "source": [
    "#we will split the training data into 2 more subsets: development and evaluation in order to first estimate the pipeline parameters\n",
    "#with the grid search and then evaluate the accuracy of the model with cross validation\n",
    "dev_data, eval_data, dev_labels, eval_labels = train_test_split(train['text'].values, train['class'].values, test_size=0.5, random_state=0)\n",
    "print len(dev_data), len(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-3. Vectorize tweets and Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "'''helper function for displaying best found features on grid_search'''\n",
    "def print_grid_search_metrics(gs):\n",
    "    print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = gs.best_params_\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "'''helper function for displaying the algorithm metrics'''\n",
    "def print_metrics(model_name, y_labels, y_predicted):\n",
    "    \n",
    "    print \"MODEL: \" + model_name\n",
    "    print 'Test Accuracy: ' + str(metrics.accuracy_score(y_labels, y_predicted))\n",
    "    \n",
    "    print '\\nClassification report:'\n",
    "    print classification_report(y_labels, y_predicted, target_names=['non-fashion tweets', 'fashion tweets'])\n",
    "    \n",
    "    print '\\nConfusion matrix:'\n",
    "    print metrics.confusion_matrix(y_labels, y_predicted)\n",
    "    \n",
    "'''helper to display the most informative features for each group'''\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_names = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top_features = zip(coefs_with_names[:n], coefs_with_names[:-(n + 1):-1])  #top features for both groups\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top_features:\n",
    "        print \"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "#add some tweets specific stop words to the built-in english list\n",
    "remove = ['amp', 'cc', 'did', 'don', 'rt', 'll', 'oh', 've', 'yes', 'let', 'going', 'via', 're', 'tweet' ]\n",
    "stop = list(ENGLISH_STOP_WORDS) + remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.snowball import *\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "class NoUrls_TfidfVectorizer(TfidfVectorizer):\n",
    "    def build_preprocessor(self):\n",
    "        url_pattern = re.compile(r'http(s?)://[\\w./]+')\n",
    "        pic_pattern = re.compile(r'pic.twitter.com/[\\w.]+')\n",
    "        preprocessor = super(NoUrls_TfidfVectorizer, self).build_preprocessor()\n",
    "        return lambda doc: (pic_pattern.sub('', url_pattern.sub('', preprocessor(doc)) ))\n",
    "    \n",
    "class NoUrls_Stemmed_TfidfVectorizer(TfidfVectorizer):\n",
    "    def build_preprocessor(self):\n",
    "        url_pattern = re.compile(r'http(s?)://[\\w./]+')\n",
    "        pic_pattern = re.compile(r'pic.twitter.com/[\\w.]+')\n",
    "        preprocessor = super(NoUrls_Stemmed_TfidfVectorizer, self).build_preprocessor()\n",
    "        return lambda doc: (pic_pattern.sub('', url_pattern.sub('', preprocessor(doc)) ))\n",
    "    \n",
    "    def build_tokenizer(self):\n",
    "        tokenizer = super(NoUrls_Stemmed_TfidfVectorizer, self).build_tokenizer()\n",
    "        return lambda doc: (stemmer.stem(w) for w in tokenizer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ngram_range: lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "#I use words and bi-grams (to consider for example \"New York\" as unique feature)\n",
    "#min_df: ignore terms that have a term frequency strictly lower than the given threshold\n",
    "#because tweets are very short, we consider min_df=1 (consider all)\n",
    "tfidf = NoUrls_TfidfVectorizer(ngram_range=(1, 2), min_df=1, stop_words=stop, strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess: rt @harpersbazaar the top 7 swimsuit trends of the season—which will you wear?   #pretty\n",
      "\n",
      "Analyze: [u'harpersbazaar', u'swimsuit', u'trends', u'season', u'wear', u'pretty', u'harpersbazaar swimsuit', u'swimsuit trends', u'trends season', u'season wear', u'wear pretty']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'harpersbazaar': 0,\n",
       " u'harpersbazaar swimsuit': 1,\n",
       " u'pretty': 2,\n",
       " u'season': 3,\n",
       " u'season wear': 4,\n",
       " u'swimsuit': 5,\n",
       " u'swimsuit trends': 6,\n",
       " u'trends': 7,\n",
       " u'trends season': 8,\n",
       " u'wear': 9,\n",
       " u'wear pretty': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example process with an specific tweet\n",
    "tweet = u'rt @harpersbazaar The top 7 swimsuit trends of the season—which will you wear? http://hbazaar.co/60109eOj pic.twitter.com/7J2hR4auMc #pretty'\n",
    "print 'Preprocess:', tfidf.build_preprocessor()(tweet)\n",
    "print\n",
    "print 'Analyze:', tfidf.build_analyzer()(tweet)\n",
    "tfidf.fit_transform([tweet])\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Bernoulli Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "Bern_classifier = BernoulliNB(binarize=None)\n",
    "Bern_pipeline = Pipeline([('tfidf', tfidf), ('clf', Bern_classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0, 'binarize': None, 'class_prior': None, 'fit_prior': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bern_classifier.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   15.5s\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:   38.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('tfidf', NoUrls_TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "            dtype=<type 'numpy.int64'>, encoding=u'utf-8',\n",
       "            input=u'content', lowercase=True, max_df=1.0,\n",
       "            max_features=None, min_df=1, ngram_range=(1, 2), norm=u'l2',\n",
       "            pr...vocabulary=None)), ('clf', BernoulliNB(alpha=1.0, binarize=None, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'tfidf__max_df': (0.8, 1.0), 'tfidf__norm': ('l1', 'l2'), 'tfidf__ngram_range': ((1, 1), (1, 2)), 'clf__alpha': (0.1, 0.5, 1)},\n",
       "       pre_dispatch='2*n_jobs', refit=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#estimate the tfidf and classifier parameters by using grid search with a nested cross validation\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.8, 1.0),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.1, 0.5, 1)\n",
    "}\n",
    "\n",
    "bern_gs = GridSearchCV(Bern_pipeline, parameters, cv=5, verbose=1, refit=False)\n",
    "bern_gs.fit(dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.901\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.1\n",
      "\ttfidf__max_df: 0.8\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "print_grid_search_metrics(bern_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build the model with the best parameters set from the grid search\n",
    "Bern_vect = NoUrls_TfidfVectorizer(ngram_range=(1, 1), min_df=1, max_df=0.8, norm='l2', stop_words=stop, strip_accents='unicode')\n",
    "Bern_classifier = BernoulliNB(alpha=0.5, binarize=None)\n",
    "Bern_pipeline = Pipeline([('tfidf', Bern_vect), ('clf', Bern_classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation accuracy: 0.90177799062\n"
     ]
    }
   ],
   "source": [
    "#score: Array of scores of the estimator for each run of the cross validation\n",
    "score = cross_val_score(Bern_pipeline, eval_data, eval_labels, cv=10)\n",
    "print \"10-fold cross validation accuracy: \" + str(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we build the final model with all the training data we have and predict the class for the testing data\n",
    "predictive_model = Bern_pipeline.fit(train['text'].values, train['class'].values)\n",
    "y_Bern_predicted = Bern_pipeline.predict(test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: Bernoulli Naive Bayes\n",
      "Test Accuracy: 0.586330935252\n",
      "\n",
      "Classification report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-fashion tweets       0.69      0.32      0.43       139\n",
      "    fashion tweets       0.56      0.86      0.67       139\n",
      "\n",
      "       avg / total       0.62      0.59      0.55       278\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 44  95]\n",
      " [ 20 119]]\n"
     ]
    }
   ],
   "source": [
    "print_metrics(\"Bernoulli Naive Bayes\", test['class'].values, y_Bern_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-8.6911\t000000         \t\t-4.0973\tjust           \n",
      "\t-8.6911\t000758         \t\t-4.1634\tgood           \n",
      "\t-8.6911\t00pm           \t\t-4.4203\tquot           \n",
      "\t-8.6911\t01             \t\t-4.4733\twork           \n",
      "\t-8.6911\t0118704263     \t\t-4.5813\tlove           \n",
      "\t-8.6911\t02             \t\t-4.5871\tlike           \n",
      "\t-8.6911\t03             \t\t-4.6023\tsleep          \n",
      "\t-8.6911\t039            \t\t-4.6317\tknow           \n",
      "\t-8.6911\t03am           \t\t-4.6661\tday            \n",
      "\t-8.6911\t05             \t\t-4.7297\tlol            \n",
      "\t-8.6911\t05ipoztuupq    \t\t-4.7442\ttoday          \n",
      "\t-8.6911\t06             \t\t-4.7522\ttime           \n",
      "\t-8.6911\t07             \t\t-4.7570\ttwitter        \n",
      "\t-8.6911\t0900           \t\t-4.7686\treally         \n",
      "\t-8.6911\t09109839513    \t\t-4.7812\tnight          \n",
      "\t-8.6911\t09am           \t\t-4.8025\tgot            \n",
      "\t-8.6911\t09left         \t\t-4.8413\tbed            \n",
      "\t-8.6911\t0e6a80e8aea4   \t\t-4.8630\tsad            \n",
      "\t-8.6911\t10037          \t\t-4.8636\tim             \n",
      "\t-8.6911\t1039           \t\t-4.9288\tthanks         \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(Bern_vect, Bern_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_tfidf = NoUrls_TfidfVectorizer(min_df=1, stop_words=stop, strip_accents='unicode')\n",
    "logistic_classifier = LogisticRegression()\n",
    "logistic_pipeline = Pipeline([('tfidf', logistic_tfidf), ('clf', logistic_classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'ovr',\n",
       " 'n_jobs': 1,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_classifier.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   17.8s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:   26.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('tfidf', NoUrls_TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "            dtype=<type 'numpy.int64'>, encoding=u'utf-8',\n",
       "            input=u'content', lowercase=True, max_df=1.0,\n",
       "            max_features=None, min_df=1, ngram_range=(1, 1), norm=u'l2',\n",
       "            pr...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'tfidf__max_df': (0.8, 1.0), 'clf__C': (1, 5, 7), 'tfidf__norm': ('l1', 'l2'), 'tfidf__ngram_range': ((1, 1), (1, 2))},\n",
       "       pre_dispatch='2*n_jobs', refit=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.8, 1.0),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C': (1, 5, 7)\n",
    "}\n",
    "\n",
    "logistic_gs = GridSearchCV(logistic_pipeline, parameters, verbose=1, refit=False)\n",
    "logistic_gs.fit(dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.905\n",
      "Best parameters set:\n",
      "\tclf__C: 7\n",
      "\ttfidf__max_df: 0.8\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "print_grid_search_metrics(logistic_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build the model with the best parameters set from the grid search\n",
    "logistic_vect = NoUrls_TfidfVectorizer(ngram_range=(1, 2), max_df=0.8, norm='l2', stop_words=stop, strip_accents='unicode')\n",
    "logistic_classifier = LogisticRegression(C=7)\n",
    "logistic_pipeline = Pipeline([('tfidf', logistic_vect), ('clf', logistic_classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation accuracy: 0.909176366727\n"
     ]
    }
   ],
   "source": [
    "#score: Array of scores of the estimator for each run of the cross validation\n",
    "score = cross_val_score(logistic_pipeline, eval_data, eval_labels, cv=10)\n",
    "print \"10-fold cross validation accuracy: \" + str(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we build the final model with all the training data we have and predict the class for the testing data\n",
    "predictive_model = logistic_pipeline.fit(train['text'].values, train['class'].values)\n",
    "y_logistic_predicted = logistic_pipeline.predict(test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: Logistic Regression\n",
      "Test Accuracy: 0.604316546763\n",
      "\n",
      "Classification report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-fashion tweets       0.70      0.37      0.48       139\n",
      "    fashion tweets       0.57      0.84      0.68       139\n",
      "\n",
      "       avg / total       0.63      0.60      0.58       278\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 51  88]\n",
      " [ 22 117]]\n"
     ]
    }
   ],
   "source": [
    "print_metrics(\"Logistic Regression\", test['class'].values, y_logistic_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-9.4061\tsunday         \t\t5.3906\tquot           \n",
      "\t-9.2219\ttomorrow       \t\t5.1342\tgood           \n",
      "\t-9.1279\tsaturday       \t\t4.9169\tsleep          \n",
      "\t-8.5340\t1st            \t\t4.7957\tthanks         \n",
      "\t-7.5426\tfriday         \t\t4.6266\tsad            \n",
      "\t-7.0390\t2nd            \t\t4.4710\tbed            \n",
      "\t-6.4407\tmonday         \t\t4.2043\ttwitter        \n",
      "\t-6.2973\tnov            \t\t4.1683\tnice           \n",
      "\t-5.9665\tsun            \t\t4.0886\thappy          \n",
      "\t-5.7004\tnovember       \t\t4.0639\tyay            \n",
      "\t-5.6654\tthursday       \t\t3.9524\tsorry          \n",
      "\t-5.6539\t4th            \t\t3.8159\tlol            \n",
      "\t-5.4749\t3rd            \t\t3.7865\tthank          \n",
      "\t-5.3899\toctober        \t\t3.7184\tmiss           \n",
      "\t-5.0321\ttonight        \t\t3.6278\texcited        \n",
      "\t-4.8878\tsat            \t\t3.4840\tugh            \n",
      "\t-4.6580\tjanuary        \t\t3.2006\tglad           \n",
      "\t-4.6291\taugust         \t\t3.1814\tlove           \n",
      "\t-4.4330\toct            \t\t3.1715\tbad            \n",
      "\t-4.2284\tmarch          \t\t3.1295\tmissed         \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(logistic_vect, logistic_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "SVM_tfidf = NoUrls_TfidfVectorizer(min_df=1, stop_words=stop, strip_accents='unicode')\n",
    "SVM_classifier = LinearSVC()\n",
    "SVM_pipeline = Pipeline([('tfidf', SVM_tfidf), ('clf', SVM_classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': True,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'loss': 'squared_hinge',\n",
       " 'max_iter': 1000,\n",
       " 'multi_class': 'ovr',\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_classifier.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   17.3s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:   28.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('tfidf', NoUrls_TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "            dtype=<type 'numpy.int64'>, encoding=u'utf-8',\n",
       "            input=u'content', lowercase=True, max_df=1.0,\n",
       "            max_features=None, min_df=1, ngram_range=(1, 1), norm=u'l2',\n",
       "            pr...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'tfidf__max_df': (0.8, 1.0), 'clf__C': (1, 5, 7), 'tfidf__norm': ('l1', 'l2'), 'tfidf__ngram_range': ((1, 1), (1, 2))},\n",
       "       pre_dispatch='2*n_jobs', refit=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.8, 1.0),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C': (1, 5, 7)\n",
    "}\n",
    "\n",
    "SVM_gs = GridSearchCV(SVM_pipeline, parameters, verbose=1, refit=False)\n",
    "SVM_gs.fit(dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.910\n",
      "Best parameters set:\n",
      "\tclf__C: 5\n",
      "\ttfidf__max_df: 0.8\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "print_grid_search_metrics(SVM_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build the model with the best parameters set from the grid search\n",
    "SVM_vect = NoUrls_TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=0.8, norm='l2', stop_words=stop, strip_accents='unicode')\n",
    "SVM_classifier = LinearSVC(C=5)\n",
    "SVM_pipeline = Pipeline([('tfidf', SVM_vect), ('clf', SVM_classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation accuracy: 0.919262696192\n"
     ]
    }
   ],
   "source": [
    "#score: Array of scores of the estimator for each run of the cross validation\n",
    "score = cross_val_score(SVM_pipeline, eval_data, eval_labels, cv=10)\n",
    "print \"10-fold cross validation accuracy: \" + str(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we build the final model with all the training data we have and predict the class for the testing data\n",
    "predictive_model = SVM_pipeline.fit(train['text'].values, train['class'].values)\n",
    "y_SVM_predicted = SVM_pipeline.predict(test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: SVM\n",
      "Test Accuracy: 0.575539568345\n",
      "\n",
      "Classification report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "non-fashion tweets       0.64      0.34      0.44       139\n",
      "    fashion tweets       0.55      0.81      0.66       139\n",
      "\n",
      "       avg / total       0.60      0.58      0.55       278\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 47  92]\n",
      " [ 26 113]]\n"
     ]
    }
   ],
   "source": [
    "print_metrics(\"SVM\", test['class'].values, y_SVM_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-4.7361\ttomorrow       \t\t2.8286\tnight tomorrow \n",
      "\t-4.5497\tsunday         \t\t2.1700\tgood           \n",
      "\t-4.4437\tsaturday       \t\t2.1569\tquot           \n",
      "\t-3.9741\t1st            \t\t2.1058\tsleep          \n",
      "\t-3.7482\tfriday         \t\t1.9974\tbed            \n",
      "\t-3.4623\t2nd            \t\t1.9591\tthanks         \n",
      "\t-3.2018\tmonday         \t\t1.9496\tnice           \n",
      "\t-3.0784\tsun            \t\t1.8708\tsad            \n",
      "\t-2.9613\tthursday       \t\t1.6482\ttwitter        \n",
      "\t-2.9308\tnov            \t\t1.6408\texcited        \n",
      "\t-2.8407\t3rd            \t\t1.6221\thappy          \n",
      "\t-2.6749\t4th            \t\t1.6017\tlol            \n",
      "\t-2.6586\tnovember       \t\t1.5790\tmiss           \n",
      "\t-2.5581\tsat            \t\t1.5524\tsorry          \n",
      "\t-2.4470\ttonight        \t\t1.5495\tready weekend  \n",
      "\t-2.4395\toctober        \t\t1.4798\tyay            \n",
      "\t-2.3204\twednesday      \t\t1.4698\tthank          \n",
      "\t-2.1830\taugust         \t\t1.4640\tugh            \n",
      "\t-2.1377\tjanuary        \t\t1.4632\tmissed         \n",
      "\t-2.1304\tmarch          \t\t1.4218\tlove           \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(SVM_vect, SVM_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Wrongly classified tweets with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_classified = y_logistic_predicted != test['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False,  True,  True, False, False, False,\n",
       "       False,  True, False,  True, False,  True,  True, False,  True,\n",
       "       False, False, False, False,  True, False, False, False,  True,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False,  True,  True, False, False,  True, False,\n",
       "        True,  True, False,  True, False, False, False, False, False,\n",
       "       False,  True, False, False,  True, False,  True, False,  True,\n",
       "        True, False, False, False, False,  True,  True, False,  True,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "        True, False, False,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True, False, False,  True,  True, False,  True,\n",
       "       False, False, False,  True, False,  True,  True,  True,  True,\n",
       "       False,  True,  True, False, False, False,  True,  True, False,\n",
       "        True, False, False,  True, False, False, False,  True,  True,\n",
       "       False,  True,  True, False,  True, False, False, False,  True,\n",
       "       False,  True,  True, False, False, False, False,  True,  True,\n",
       "        True, False, False, False, False, False,  True, False,  True,\n",
       "        True,  True,  True, False, False,  True, False,  True,  True,\n",
       "        True,  True, False, False, False, False,  True, False, False,\n",
       "        True,  True,  True,  True,  True, False, False,  True, False,\n",
       "       False,  True, False,  True,  True,  True,  True, False, False,\n",
       "       False,  True, False,  True,  True, False,  True, False, False,\n",
       "        True, False,  True, False, False,  True,  True,  True,  True,\n",
       "        True, False, False, False,  True, False,  True,  True, False,\n",
       "       False, False, False,  True, False, False, False,  True,  True,\n",
       "        True, False, False, False,  True, False,  True,  True, False,\n",
       "       False, False,  True, False, False, False,  True, False, False,\n",
       "       False,  True,  True,  True, False, False,  True, False,  True,\n",
       "       False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        True, False, False, False, False, False,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True, False,  True], dtype=bool)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamieconnelly/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 1784 but corresponding boolean dimension is 278\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "wrong_data = test_data[wrong_classified == True]\n",
    "wrong_labels = y_logistic_predicted[wrong_classified == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "false_positive = wrong_data[wrong_labels == 1] #labeled as 1 (fashion) when should be 0 (non-fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(false_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#example tweet\n",
    "false_positive[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "false_negative = wrong_data[wrong_labels == 0]  #labeled as 0 (non-fashion) when they should belong to 1 (fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#example tweet\n",
    "false_negative[24]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
