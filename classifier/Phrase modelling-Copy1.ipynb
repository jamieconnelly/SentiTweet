{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "p.options.display.max_colwidth = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = p.read_csv('./sub_obj_data/train_new.csv', usecols=(['class', 'text'])).dropna()\n",
    "test  = p.read_csv('./sub_obj_data/test_ds.csv', usecols=(['class', 'text'])).dropna()\n",
    "train = train.reindex(np.random.permutation(train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "import re\n",
    "import htmlentitydefs\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class Preprocessor():\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.stopwords = sw.words('english')\n",
    "        self.word_re = word_re\n",
    "        self.emoticon_re = emoticon_re\n",
    "        self.html_entity_digit_re = html_entity_digit_re\n",
    "        self.html_entity_alpha_re = html_entity_alpha_re\n",
    "        self.amp = amp\n",
    "        self.punct_re = punct_re\n",
    "        self.negation_re = negation_re\n",
    "        self.url_re = url_re\n",
    "        self.rep_char_re = rep_char_re\n",
    "        self.hashtag_re = hashtag_re\n",
    "        self.user_tag_re = user_tag_re\n",
    "        self.top_bigrams = self.get_bigrams(corpus)\n",
    "        \n",
    "    def helper(self, tweet):\n",
    "        tweet = self.__html2unicode(tweet)\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "        vect = []\n",
    "        for t in tokens:\n",
    "            if t in self.stopwords or t in string.punctuation:\n",
    "                continue\n",
    "            vect.append(t)\n",
    "        \n",
    "        return vect\n",
    "        \n",
    "    def get_bigrams(self, corpus):\n",
    "        tokens = [] #[token for sublist in corpus for token in sublist]\n",
    "        for tweet in corpus:\n",
    "            tokens.append(self.helper(tweet.decode('utf-8').lower()))\n",
    "        tokens = [val for sublist in tokens for val in sublist]\n",
    "        \n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        finder = BigramCollocationFinder.from_words(tokens)\n",
    "        finder.apply_freq_filter(5)\n",
    "        return finder.nbest(bigram_measures.likelihood_ratio, 1000)\n",
    "        \n",
    "\n",
    "    def load_acrynoms(self):\n",
    "        with open('../data/acrynom.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            slang = dict((rows[0], rows[1]) for rows in reader)\n",
    "            return slang\n",
    "\n",
    "\n",
    "    def normalise(self, tokens):\n",
    "        vect = []\n",
    "        \n",
    "        bigrams = ngrams(tokens, 2)\n",
    "        \n",
    "#         for bigram in bigrams:\n",
    "#             bigram_lower = (bigram[0].lower(), bigram[1].lower())\n",
    "#             if bigram_lower in self.top_bigrams:\n",
    "#                 if bigram_lower[0] not in self.stopwords or bigram_lower[1] not in self.stopwords :\n",
    "#                     vect.append(bigram_lower[0] + '_' + bigram_lower[1])\n",
    "        \n",
    "        \n",
    "        for t in tokens:\n",
    "#             if (t in self.stopwords and\n",
    "#                     not self.negation_re.match(t)):\n",
    "#                 continue\n",
    "            if t in self.stopwords or t in string.punctuation:\n",
    "                continue\n",
    "            \n",
    "            if not self.emoticon_re.search(t):\n",
    "                t = t.lower()\n",
    "            \n",
    "            t = self.rep_char_re.sub(r'\\1', t)\n",
    "            t = self.url_re.sub('_URL', t)\n",
    "            t = self.hashtag_re.sub('_HASH', t)\n",
    "            t = self.user_tag_re.sub('_USER', t)\n",
    "            \n",
    "            vect.append(t)\n",
    "        \n",
    "        return vect\n",
    "\n",
    "    def tokenise(self, tweet):\n",
    "        tweet = self.__html2unicode(tweet)\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "        return self.normalise(tokens)\n",
    "\n",
    "    def ensure_unicode(self, tweet):\n",
    "        try:\n",
    "            return unicode(tweet)\n",
    "        except UnicodeDecodeError:\n",
    "            tweet = str(tweet).encode('string_escape')\n",
    "            return unicode(tweet)\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        This function is curtosy of Christopher Potts\n",
    "        http://sentiment.christopherpotts.net/index.html\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(self.html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(self.html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x: x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:\n",
    "                s = s.replace(ent,\n",
    "                              unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass\n",
    "            s = s.replace(self.amp, \" and \")\n",
    "        return s\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "    This file is based on the work of Christopher Potts\n",
    "    however the file has been altered and extended for\n",
    "    my purposes\n",
    "    http://sentiment.christopherpotts.net/index.html\n",
    "\"\"\"\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?\n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?\n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*\n",
    "      \\d{4}          # base\n",
    "    )\"\"\",\n",
    "    # Emoticons:\n",
    "    emoticon_string,\n",
    "    # HTML tags:\n",
    "    r'<[^>]+>',\n",
    "    # Twitter username:\n",
    "    r'(?:@[\\w_]+)',\n",
    "    # Links\n",
    "    r'http\\S+',\n",
    "    # Twitter hashtags:\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                    # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "negation_words = (\n",
    "    \"\"\"\n",
    "    (?x)(?:\n",
    "    ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "        havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "     )$\n",
    "    )\n",
    "    |\n",
    "    n't\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# ######################################################################\n",
    "\n",
    "word_re = re.compile(r'(%s)' % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "html_entity_digit_re = re.compile(r'&#\\d+;')\n",
    "html_entity_alpha_re = re.compile(r'&\\w+;')\n",
    "amp = \"&amp;\"\n",
    "punct_re = re.compile(\"^[.:;!?]$\")\n",
    "negation_re = re.compile(negation_words)\n",
    "url_re = re.compile(r'http\\S+')\n",
    "rep_char_re = re.compile(r'(\\w)\\1{3,}')\n",
    "hashtag_re = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "user_tag_re = re.compile(r'(?:@[\\w_]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_evaluate(X, y, X_test, y_test, outpath=None):\n",
    "\n",
    "    def preprocess(s):\n",
    "        return preprocessor.tokenise(s)\n",
    "\n",
    "    # Initialise transformers/estimators\n",
    "    # clf = MultinomialNB()\n",
    "#     clf = SGDClassifier()\n",
    "    clf = LogisticRegression()\n",
    "    preprocessor = Preprocessor(X_test)\n",
    "    vec = TfidfVectorizer(tokenizer=preprocess,\n",
    "                          lowercase=False,\n",
    "                          ngram_range=(1, 1),\n",
    "                          max_features=5000)\n",
    "\n",
    "    # Build model\n",
    "    print(\"Building model\")\n",
    "    matrix = vec.fit_transform(X)\n",
    "    clf.fit(matrix, y)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    matrix = vec.transform(X_test)\n",
    "    y_pred = clf.predict(matrix)\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print np.mean(y_pred == y_test)\n",
    "    print cm(y_test, y_pred)\n",
    "    print(clsr(y_test, y_pred, target_names=['obj', 'sub']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.672661870504\n",
      "[[ 85  54]\n",
      " [ 37 102]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.70      0.61      0.65       139\n",
      "        sub       0.65      0.73      0.69       139\n",
      "\n",
      "avg / total       0.68      0.67      0.67       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_and_evaluate(train['text'].values, train['class'].values, test['text'].values, test['class'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make corpus of sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.676258992806\n",
      "[[ 85  54]\n",
      " [ 36 103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.70      0.61      0.65       139\n",
      "        sub       0.66      0.74      0.70       139\n",
      "\n",
      "avg / total       0.68      0.68      0.67       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_and_evaluate(train['text'].values, train['class'].values, test['text'].values, test['class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_bigrams(myString):\n",
    "#     tokenizer = Preprocessor()\n",
    "#     tokens = tokenizer.tokenise(myString)\n",
    "#     stemmer = PorterStemmer()\n",
    "#     bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "#     bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 500)\n",
    "\n",
    "#     for bigram_tuple in bigrams:\n",
    "#         x = \"%s %s\" % bigram_tuple\n",
    "#         tokens.append(x)\n",
    "\n",
    "#     return ['_'.join([stemmer.stem(w).lower() for w in x.split() if w not in string.punctuation]) for x in tokens ]\n",
    "\n",
    "# get_bigrams('@some royal flying rumble basketball edition, YAAY!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'3_hours', u'3', u'hours', u'midnight', u'_USER']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Preprocessor(train['text'])\n",
    "s = tokenizer.tokenise('3 hours to midnight @hel')\n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'twitter', u'com'),\n",
       " (u'new', u'york'),\n",
       " (u'last', u'night'),\n",
       " (u'amanda', u'knox'),\n",
       " (u\"i'm\", u'going'),\n",
       " (u'pic', u'twitter'),\n",
       " (u'michael', u'jackson'),\n",
       " (u'real', u'madrid'),\n",
       " (u'\\xef', u'\\xbf'),\n",
       " (u'tampa', u'bay'),\n",
       " (u'nuit', u'blanche'),\n",
       " (u'footymad', u'attempt'),\n",
       " (u\"can't\", u'wait'),\n",
       " (u'droid', u'bionic'),\n",
       " (u'constitution', u'hall'),\n",
       " (u'jason', u'vernau'),\n",
       " (u'attempt', u'assist'),\n",
       " (u'abu', u'dhabi'),\n",
       " (u'daily', u'zap'),\n",
       " (u'fu', u'panda'),\n",
       " (u'andy', u'rooney'),\n",
       " (u'port', u'abu'),\n",
       " (u'kung', u'fu'),\n",
       " (u'bayer', u'leverkusen'),\n",
       " (u'pj', u'harvey'),\n",
       " (u'assist', u'form'),\n",
       " (u'brooklyn', u'bridge'),\n",
       " (u'good', u'morning'),\n",
       " (u':/', u'www'),\n",
       " (u'tony', u'romo'),\n",
       " (u'trayvon', u'martin'),\n",
       " (u'form', u'guide'),\n",
       " (u'subtitles', u'tentatively'),\n",
       " (u'yushin', u'okami'),\n",
       " (u'saturday', u'night'),\n",
       " (u'2d', u'3d'),\n",
       " (u\"don't\", u'know'),\n",
       " (u'ghost', u'rider'),\n",
       " (u'monday', u'night'),\n",
       " (u'indonesian', u'subtitles'),\n",
       " (u'windows', u'7'),\n",
       " (u'arrival', u'departure'),\n",
       " (u'fm', u'preview'),\n",
       " (u'cloud', u'expo'),\n",
       " (u'3d', u'indonesian'),\n",
       " (u'tim', u'cook'),\n",
       " (u'looking', u'forward'),\n",
       " (u'tentatively', u'scheduled'),\n",
       " (u'looks', u'like'),\n",
       " (u'lady', u'gaga'),\n",
       " (u'white', u'arrows'),\n",
       " (u'tiger', u'woods'),\n",
       " (u'red', u'cross'),\n",
       " (u'andre', u'villas-boas'),\n",
       " (u'crows', u'nest'),\n",
       " (u'2012', u'footymad'),\n",
       " (u'college', u'id'),\n",
       " (u'feel', u'like'),\n",
       " (u'american', u'hockey'),\n",
       " (u'ceo', u'tim'),\n",
       " (u'postal', u'service'),\n",
       " (u'join', u'us'),\n",
       " (u'tomorrow', u'night'),\n",
       " (u'ice', u'cream'),\n",
       " (u'dar', u'constitution'),\n",
       " (u'triangle', u'sighting'),\n",
       " (u'black', u'triangle'),\n",
       " (u'elvis', u'presley'),\n",
       " (u'bay', u'buccaneers'),\n",
       " (u'adrienne', u'arsht'),\n",
       " (u\"cnblue's\", u'yonghwa'),\n",
       " (u'kristin', u'chenoweth'),\n",
       " (u'good', u'luck'),\n",
       " (u'san', u'francisco'),\n",
       " (u'san', u'siro'),\n",
       " (u'playstation', u'vita'),\n",
       " (u'social', u'media'),\n",
       " (u'premier', u'league'),\n",
       " (u'jennifer', u'aniston'),\n",
       " (u'pavol', u'demitra'),\n",
       " (u'rest', u'peace'),\n",
       " (u'red', u'carpet'),\n",
       " (u'robbie', u'keane'),\n",
       " (u'boise', u'state'),\n",
       " (u'tues', u'16'),\n",
       " (u'kinder', u'morgan'),\n",
       " (u'wanna', u'go'),\n",
       " (u'bit', u'ly'),\n",
       " (u'conrad', u'murray'),\n",
       " (u'black', u'dahlia'),\n",
       " (u'bullet', u'valentine'),\n",
       " (u'ricky', u'ponting'),\n",
       " (u'night', u'football'),\n",
       " (u'hockey', u'league'),\n",
       " (u'jose', u'mourinho'),\n",
       " (u'klub', u'ace'),\n",
       " (u'spring', u'break'),\n",
       " (u'sorry', u'hear'),\n",
       " (u'knicks', u'vs'),\n",
       " (u'anderson', u'silva'),\n",
       " (u'monta', u'ellis'),\n",
       " (u'@blitzmegaplex', u'kung'),\n",
       " (u'sbs', u'k-pop'),\n",
       " (u'carling', u'cup'),\n",
       " (u'london', u\"children's\"),\n",
       " (u'going', u'bed'),\n",
       " (u'eli', u'manning'),\n",
       " (u'panda', u'2'),\n",
       " (u'destination', u'5'),\n",
       " (u'planet', u'apes'),\n",
       " (u'emmanuel', u'adebayor'),\n",
       " (u'nipsey', u'hussle'),\n",
       " (u'scheduled', u'open'),\n",
       " (u'singapore', u'slingers'),\n",
       " (u'international', u'committee'),\n",
       " (u'wish', u'could'),\n",
       " (u'wichita', u'wild'),\n",
       " (u'nokia', u'n9'),\n",
       " (u'wide', u'awake'),\n",
       " (u'world', u'bank'),\n",
       " (u'dave', u'grohl'),\n",
       " (u'daytona', u'500'),\n",
       " (u'marie', u'colvin'),\n",
       " (u'maya', u'rudolph'),\n",
       " (u'melissa', u'mccarthy'),\n",
       " (u'spread', u'word'),\n",
       " (u'dhabi', u'time'),\n",
       " (u'utc', u'time'),\n",
       " (u'open', u'tues'),\n",
       " (u'george', u'zimmerman'),\n",
       " (u'final', u'destination'),\n",
       " (u'getting', u'ready'),\n",
       " (u'comedy', u'central'),\n",
       " (u'machine', u'gun'),\n",
       " (u'jungle', u'island'),\n",
       " (u'rise', u'planet'),\n",
       " (u'aston', u'villa'),\n",
       " (u'raspberry', u'pi'),\n",
       " (u'k-pop', u'super'),\n",
       " (u\"can't\", u'sleep'),\n",
       " (u'2nd', u'half'),\n",
       " (u'independence', u'day'),\n",
       " (u'b2st', u'kikwang'),\n",
       " (u'jury', u'selection'),\n",
       " (u'mohamed', u'morsi'),\n",
       " (u'2', u'2d'),\n",
       " (u'tiffany', u\"cnblue's\"),\n",
       " (u'u', u'postal'),\n",
       " (u'fall', u'asleep'),\n",
       " (u'united', u'states'),\n",
       " (u'las', u'vegas'),\n",
       " (u'arsht', u'center'),\n",
       " (u'book', u'swap'),\n",
       " (u'nicki', u'minaj'),\n",
       " (u'morgan', u'state'),\n",
       " (u'apple', u'ceo'),\n",
       " (u'steve', u'jobs'),\n",
       " (u'cherry', u'moon'),\n",
       " (u'san', u'antonio'),\n",
       " (u'jason', u'varitek'),\n",
       " (u'deus', u'ex'),\n",
       " (u'lil', u'wayne'),\n",
       " (u'roger', u'sanchez'),\n",
       " (u'black', u'friday'),\n",
       " (u'winter', u'storm'),\n",
       " (u'departure', u'ship'),\n",
       " (u'ps', u'vita'),\n",
       " (u'club', u'bodi'),\n",
       " (u'rick', u'stein'),\n",
       " (u'yonghwa', u'b2st'),\n",
       " (u'vs', u'heat'),\n",
       " (u'map', u'puzzle'),\n",
       " (u'spirit', u'vengeance'),\n",
       " (u'sweet', u'dreams'),\n",
       " (u'breaking', u'news'),\n",
       " (u'august', u'isk'),\n",
       " (u'david', u'miliband'),\n",
       " (u'time', u'vessel'),\n",
       " (u'mysterious', u'island'),\n",
       " (u'nick', u'watney'),\n",
       " (u'royal', u'rumble'),\n",
       " (u'axl', u'rose'),\n",
       " (u'bayern', u'munich'),\n",
       " (u'60', u'minutes'),\n",
       " (u'enter', u'competition'),\n",
       " (u'gun', u'kelly'),\n",
       " (u'international', u'cloud'),\n",
       " (u'time', u'arrival'),\n",
       " (u'16', u'august'),\n",
       " (u'\\u2026', u'via'),\n",
       " (u'cream', u'sandwich'),\n",
       " (u'#trayvon', u'daily'),\n",
       " (u'make', u'sure'),\n",
       " (u'james', u'murphy'),\n",
       " (u'news', u'buzz'),\n",
       " (u\"don't\", u'want'),\n",
       " (u'guardian', u'co'),\n",
       " (u'memory', u'trayvon'),\n",
       " (u'tom', u'segal'),\n",
       " (u'world', u'cup'),\n",
       " (u'nov', u'1'),\n",
       " (u'co', u'uk'),\n",
       " (u'washington', u'monument'),\n",
       " (u'volleyball', u'game'),\n",
       " (u'neighborhood', u'theatre'),\n",
       " (u'university', u'denver'),\n",
       " (u'super', u'concert'),\n",
       " (u'careless', u'world'),\n",
       " (u'set', u'nightclub'),\n",
       " (u'martin', u'killed'),\n",
       " (u'filthy', u'rich'),\n",
       " (u'bell', u'center'),\n",
       " (u'thursday', u'night'),\n",
       " (u\"who's\", u'going'),\n",
       " (u'committee', u'red'),\n",
       " (u'plane', u'crash'),\n",
       " (u'departure', u'time'),\n",
       " (u'www', u'guardian'),\n",
       " (u'dahlia', u'murder'),\n",
       " (u'michael', u'clarke'),\n",
       " (u'sore', u'throat'),\n",
       " (u'@alfredoflores', u'follow'),\n",
       " (u'follow', u'@amandawanxo'),\n",
       " (u'puzzle', u'follow'),\n",
       " (u'san', u'diego'),\n",
       " (u'bill', u'murray'),\n",
       " (u'nightclub', u'miami'),\n",
       " (u'rich', u'mix'),\n",
       " (u'preview', u'west'),\n",
       " (u'presidents', u'day'),\n",
       " (u'inc', u'ceo'),\n",
       " (u\"children's\", u'map'),\n",
       " (u'red', u'sox'),\n",
       " (u'october', u'31st'),\n",
       " (u'academy', u'awards'),\n",
       " (u'rising', u'stars'),\n",
       " (u\"i'm\", u'sorry'),\n",
       " (u'backstage', u'live'),\n",
       " (u':/', u'bit'),\n",
       " (u'session', u'11th'),\n",
       " (u'28', u'apr'),\n",
       " (u'apple', u'inc'),\n",
       " (u'iron', u'man'),\n",
       " (u'miami', u'february'),\n",
       " (u'black', u'forest'),\n",
       " (u'london', u'conference'),\n",
       " (u'front', u'line'),\n",
       " (u'new', u'ct'),\n",
       " (u'lamb', u'god'),\n",
       " (u'norwich', u'city'),\n",
       " (u'let', u'know'),\n",
       " (u\"we're\", u'going'),\n",
       " (u'1st', u'quarter'),\n",
       " (u'brand', u'new'),\n",
       " (u'rt', u'closes'),\n",
       " (u'plan', u'b'),\n",
       " (u'2', u'days'),\n",
       " (u'october', u'23rd'),\n",
       " (u'music', u'festival'),\n",
       " (u'days', u'ago'),\n",
       " (u'competition', u'win'),\n",
       " (u'media', u'week'),\n",
       " (u'rugby', u'world'),\n",
       " (u'feel', u'better'),\n",
       " (u'pancake', u'day'),\n",
       " (u'id', u'til'),\n",
       " (u'first', u'time'),\n",
       " (u'come', u'back'),\n",
       " (u'via', u'@youtube'),\n",
       " (u'david', u'taylor'),\n",
       " (u'p', u'g'),\n",
       " (u'york', u'knicks'),\n",
       " (u'big', u'brother'),\n",
       " (u'storm', u'watch'),\n",
       " (u'mexican', u'open'),\n",
       " (u\"let's\", u'see'),\n",
       " (u'rt', u'@blitzmegaplex'),\n",
       " (u'11th', u'international'),\n",
       " (u'february', u'25th'),\n",
       " (u'all-star', u'game'),\n",
       " (u'nov', u'2'),\n",
       " (u'high', u'school'),\n",
       " (u'good', u'night'),\n",
       " (u'november', u'26'),\n",
       " (u'cross', u'said'),\n",
       " (u'4th', u'july'),\n",
       " (u'jim', u'white'),\n",
       " (u\"i've\", u'seen'),\n",
       " (u'sunday', u'night'),\n",
       " (u'apr', u'2012'),\n",
       " (u'4th', u'quarter'),\n",
       " (u\"don't\", u'forget'),\n",
       " (u'daily', u'tweet'),\n",
       " (u'game', u'tomorrow'),\n",
       " (u'tryna', u'get'),\n",
       " (u'last', u'day'),\n",
       " (u'gonna', u'watch'),\n",
       " (u'3rd', u'round'),\n",
       " (u'late', u'night'),\n",
       " (u'sep', u'2012'),\n",
       " (u'say', u'hello'),\n",
       " (u'safe', u'house'),\n",
       " (u'tomorrow', u'morning'),\n",
       " (u'looked', u'like'),\n",
       " (u'long', u'beach'),\n",
       " (u'family', u'guy'),\n",
       " (u\"i'm\", u'sure'),\n",
       " (u\"i've\", u'got'),\n",
       " (u\"didn't\", u'get'),\n",
       " (u'nov', u'10'),\n",
       " (u'getting', u'sick'),\n",
       " (u'saturday', u'kitchen'),\n",
       " (u'vernau', u'live'),\n",
       " (u'take', u'care'),\n",
       " (u'25th', u'2012'),\n",
       " (u'feels', u'like'),\n",
       " (u'go', u'see'),\n",
       " (u'cook', u'monday'),\n",
       " (u'oct', u'28'),\n",
       " (u'good', u'idea'),\n",
       " (u'australia', u'day'),\n",
       " (u\"i'm\", u'glad'),\n",
       " (u'oct', u'30'),\n",
       " (u'vernau', u'back'),\n",
       " (u'time', u'nov'),\n",
       " (u'2012', u'jason'),\n",
       " (u'1st', u'time'),\n",
       " (u'going', u'sleep'),\n",
       " (u'win', u'london'),\n",
       " (u'watch', u'hills'),\n",
       " (u'league', u'team'),\n",
       " (u'2', u'2012'),\n",
       " (u'3', u'hours'),\n",
       " (u'think', u'need'),\n",
       " (u'wants', u'go'),\n",
       " (u'coming', u'soon'),\n",
       " (u'days', u'till'),\n",
       " (u'new', u'album'),\n",
       " (u'nov', u'15'),\n",
       " (u'said', u'thursday'),\n",
       " (u'gonna', u'go'),\n",
       " (u'im', u'sorry'),\n",
       " (u'oh', u'well'),\n",
       " (u'maybe', u\"i'll\"),\n",
       " (u'3', u'days'),\n",
       " (u'pretty', u'good'),\n",
       " (u\"don't\", u'think'),\n",
       " (u'may', u'rest'),\n",
       " (u'next', u'friday'),\n",
       " (u'even', u'though'),\n",
       " (u'follow', u'rt'),\n",
       " (u'dont', u'know'),\n",
       " (u'still', u'waiting'),\n",
       " (u'friday', u'night'),\n",
       " (u'man', u'3'),\n",
       " (u'need', u'get'),\n",
       " (u\"i'm\", u'thinking'),\n",
       " (u'team', u'week'),\n",
       " (u'one', u'lions'),\n",
       " (u'make', u'final'),\n",
       " (u'party', u'house'),\n",
       " (u'u', u'r'),\n",
       " (u\"i'm\", u'excited'),\n",
       " (u'sounds', u'like'),\n",
       " (u\"i'm\", u'tired'),\n",
       " (u'2012', u'oct'),\n",
       " (u\"i'm\", u'gonna'),\n",
       " (u'november', u'4th'),\n",
       " (u'two', u'days'),\n",
       " (u'come', u'see'),\n",
       " (u\"can't\", u'find'),\n",
       " (u'hope', u'feel'),\n",
       " (u'go', u'bed'),\n",
       " (u'good', u'day'),\n",
       " (u'one', u'day'),\n",
       " (u'live', u'set'),\n",
       " (u'1st', u'half'),\n",
       " (u'ago', u'may'),\n",
       " (u'love', u'love'),\n",
       " (u'might', u'go'),\n",
       " (u'want', u'go'),\n",
       " (u'new', u'year'),\n",
       " (u'got', u'home'),\n",
       " (u'may', u'want'),\n",
       " (u'next', u'wednesday'),\n",
       " (u'football', u'game'),\n",
       " (u'2', u'hours'),\n",
       " (u'new', u'post'),\n",
       " (u'opening', u'night'),\n",
       " (u'next', u'week'),\n",
       " (u'go', u'buy'),\n",
       " (u'last', u'year'),\n",
       " (u'another', u'day'),\n",
       " (u'starts', u'tomorrow'),\n",
       " (u'monday', u'morning'),\n",
       " (u\"didn't\", u'know'),\n",
       " (u'1st', u'10'),\n",
       " (u'make', u'last'),\n",
       " (u\"it's\", u'going'),\n",
       " (u'oh', u'oh'),\n",
       " (u'3rd', u'time'),\n",
       " (u'next', u'saturday'),\n",
       " (u'school', u'tomorrow'),\n",
       " (u\"don't\", u'like'),\n",
       " (u\"don't\", u'miss'),\n",
       " (u'game', u'friday'),\n",
       " (u'go', u'sleep'),\n",
       " (u'time', u'season'),\n",
       " (u'first', u'day'),\n",
       " (u'get', u'early'),\n",
       " (u\"i'm\", u'still'),\n",
       " (u'love', u'show'),\n",
       " (u'game', u'tonight'),\n",
       " (u'next', u'thursday'),\n",
       " (u'get', u'ready'),\n",
       " (u'feel', u'good'),\n",
       " (u'get', u'tickets'),\n",
       " (u'day', u'tomorrow'),\n",
       " (u'anyone', u'going'),\n",
       " (u'go', u'school'),\n",
       " (u'night', u'live'),\n",
       " (u'saturday', u'morning'),\n",
       " (u'back', u'work'),\n",
       " (u'...', u\"i'm\"),\n",
       " (u'last', u'friday'),\n",
       " (u\"i'm\", u'sad'),\n",
       " (u'got', u'back'),\n",
       " (u\"i'm\", u'happy'),\n",
       " (u'want', u'see'),\n",
       " (u'long', u'day'),\n",
       " (u'would', u'like'),\n",
       " (u'days', u'go'),\n",
       " (u'day', u'today'),\n",
       " (u\"can't\", u'see'),\n",
       " (u'got', u'one'),\n",
       " (u'game', u'saturday'),\n",
       " (u'day', u'school'),\n",
       " (u'big', u'day'),\n",
       " (u'last', u'sunday'),\n",
       " (u'early', u'tomorrow'),\n",
       " (u\"it's\", u'good'),\n",
       " (u'may', u'know'),\n",
       " (u'2nd', u'time'),\n",
       " (u\"it's\", u'like'),\n",
       " (u'may', u'get'),\n",
       " (u'get', u'back'),\n",
       " (u'...', u'need'),\n",
       " (u'im', u'going'),\n",
       " (u'time', u'get'),\n",
       " (u'last', u'one'),\n",
       " (u\"it's\", u'time'),\n",
       " (u'..', u'still'),\n",
       " (u'want', u'get'),\n",
       " (u'go', u'work'),\n",
       " (u'going', u'back'),\n",
       " (u'still', u'time'),\n",
       " (u'thursday', u'...'),\n",
       " (u'well', u\"i'm\"),\n",
       " (u'work', u'today'),\n",
       " (u'b', u'...'),\n",
       " (u'go', u'back'),\n",
       " (u'last', u'time'),\n",
       " (u'today', u'...'),\n",
       " (u'tomorrow', u\"it's\"),\n",
       " (u'haha', u'...'),\n",
       " (u'show', u'tomorrow'),\n",
       " (u'good', u'time'),\n",
       " (u'today', u'see'),\n",
       " (u'may', u'may'),\n",
       " (u'...', u'oh'),\n",
       " (u'know', u\"i'm\"),\n",
       " (u'tomorrow', u'last'),\n",
       " (u'see', u'tomorrow'),\n",
       " (u'...', u'feel'),\n",
       " (u'tomorrow', u'go'),\n",
       " (u'like', u\"i'm\"),\n",
       " (u'..', u\"i'm\"),\n",
       " (u'...', u\"don't\"),\n",
       " (u'season', u'...'),\n",
       " (u'game', u'...'),\n",
       " (u'sunday', u'...'),\n",
       " (u'...', u\"can't\"),\n",
       " (u'tomorrow', u'see'),\n",
       " (u'tomorrow', u'may'),\n",
       " (u'...', u'well'),\n",
       " (u'tomorrow', u'get'),\n",
       " (u'saturday', u'...'),\n",
       " (u'monday', u'...'),\n",
       " (u'time', u'...'),\n",
       " (u'...', u'think'),\n",
       " (u'...', u'still'),\n",
       " (u'friday', u'...'),\n",
       " (u'...', u\"it's\"),\n",
       " (u'work', u'...'),\n",
       " (u'tomorrow', u\"i'm\"),\n",
       " (u'...', u'got'),\n",
       " (u'one', u'...'),\n",
       " (u'tomorrow', u'...'),\n",
       " (u'back', u'...'),\n",
       " (u'day', u'...'),\n",
       " (u'tonight', u'...'),\n",
       " (u'night', u'...')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.top_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "text = \"Hi How are you? i am fine and you\"\n",
    "token=nltk.word_tokenize(text)\n",
    "bigrams=ngrams(token,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Hi', u'How', u'are', u'you', u'?', u'i', u'am', u'fine', u'and', u'you']\n",
      "(u'Hi', u'How')\n",
      "(u'How', u'are')\n",
      "(u'are', u'you')\n",
      "(u'you', u'?')\n",
      "(u'?', u'i')\n",
      "(u'i', u'am')\n",
      "(u'am', u'fine')\n",
      "(u'fine', u'and')\n",
      "(u'and', u'you')\n"
     ]
    }
   ],
   "source": [
    "print token\n",
    "for b in bigrams:\n",
    "    print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
