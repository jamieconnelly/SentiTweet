{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of Subjective/Objective tweets\n",
    "## Features: \n",
    "- tfidf count\n",
    "- POS tags in BoW fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as p\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = p.read_csv('../sub_obj_data/train_new.csv', usecols=(['class', 'text'])).dropna()\n",
    "test  = p.read_csv('../sub_obj_data/test_ds.csv', usecols=(['class', 'text'])).dropna()\n",
    "train = train.reindex(np.random.permutation(train.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor class\n",
    "Helper class which tokenises tweets and creates additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"Tweet tokenisor and feature extractor.\n",
    "\n",
    "       Attributes:\n",
    "           feats (dict of lists): Contains counts for lexicon features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stopwords = list(sw.words('english'))\n",
    "        self.word_re = word_re\n",
    "        self.emoticon_re = emoticon_re\n",
    "        self.url_re = url_re\n",
    "        self.rep_char_re = rep_char_re\n",
    "        self.hashtag_re = hashtag_re\n",
    "        self.user_tag_re = user_tag_re\n",
    "        self.lexicon = self._load_lexicon()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.feats = {'pos': [], 'neg': []}\n",
    "\n",
    "    def tokenise(self, tweet, pos_tags=False):\n",
    "        \"\"\"Tweet tokenisor method.\n",
    "\n",
    "            Args:\n",
    "                tweet (str): Text of a tweet.\n",
    "                lexicon_feats (bool): Whether to include lexicon\n",
    "                    features or not.\n",
    "\n",
    "            Returns:\n",
    "                Returns list of str.\n",
    "        \"\"\"\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "        return self._normalise(tokens, pos_tags)\n",
    "\n",
    "    def reset_feats(self):\n",
    "        \"\"\"Re-initialises the feats attribute.\"\"\"\n",
    "        self.feats = {k: [] for k, v in self.feats.iteritems()}\n",
    "\n",
    "    def normalise_vect(self):\n",
    "        \"\"\"Normalises each value in the feats attribute.\"\"\"\n",
    "        max_val = 0\n",
    "        for v in self.feats.itervalues():\n",
    "            temp = max(map(lambda x: x[0], v))\n",
    "            if temp > max_val:\n",
    "                max_val = temp\n",
    "\n",
    "        for k, v in self.feats.iteritems():\n",
    "            self.feats[k] = map(lambda x: [0] if x[0] == 0 else [(x[0] / max_val) * 15], v)\n",
    "\n",
    "    def _load_lexicon(self):\n",
    "        with open('../app/data/lexicon.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            return dict((rows[2], rows[5]) for rows in reader)\n",
    "\n",
    "    def _pos_tags(self, tokens):\n",
    "        TAG_MAP = [\"NN\", \"NNP\", \"NNS\", \"VBP\", \"VB\", \"VBD\", 'VBG',\n",
    "                   \"VBN\", \"VBZ\", \"MD\",\"UH\", \"PRP\", \"PRP$\"]\n",
    "        tags = pos_tag(tokens)\n",
    "        return [tag[1] for tag in tags if tag[1] in TAG_MAP]\n",
    "\n",
    "    def _normalise(self, tokens, pos_tags):\n",
    "\n",
    "        token_list = []\n",
    "\n",
    "        for t in tokens:\n",
    "            # Ignore stopwords\n",
    "            if t in self.stopwords:\n",
    "                continue\n",
    "\n",
    "            # lowercase all tokens except for emoticons\n",
    "            if not self.emoticon_re.search(t):\n",
    "                t = t.lower()\n",
    "\n",
    "            # Normalise tokens\n",
    "            t = self.rep_char_re.sub(r'\\1', t)\n",
    "            t = self.url_re.sub('_URL', t)\n",
    "            t = self.hashtag_re.sub('_HASH', t)\n",
    "            t = self.user_tag_re.sub('_USER', t)\n",
    "\n",
    "            # Get token's stem and append it to the list\n",
    "            token_list.append(self.stemmer.stem(t))\n",
    "\n",
    "        # Get list of pos tags and append to token_list\n",
    "        if pos_tags:\n",
    "            tags = self._pos_tags(tokens)\n",
    "            token_list = tags + token_list\n",
    "        return token_list\n",
    "\n",
    "    def _lexicon_lookup(self, tokens):\n",
    "        # Initialise new 'row' to list\n",
    "        for k, v in self.feats.iteritems():\n",
    "            self.feats[k].append([0])\n",
    "\n",
    "        idx = len(self.feats['pos']) - 1\n",
    "\n",
    "        # Check if token is in lexicon dictionary.\n",
    "        # If it is increment pos/neg feature count\n",
    "        for t in tokens:\n",
    "            t = t.lower()\n",
    "            if t in self.lexicon:\n",
    "                if self.lexicon[t] == '4':\n",
    "                    self.feats['pos'][idx][0] += 1\n",
    "                elif self.lexicon[t] == '0':\n",
    "                    self.feats['neg'][idx][0] += 1\n",
    "\n",
    "\"\"\"\n",
    "    This file is based on the work of Christopher Potts.\n",
    "    However, the file has been altered and extended for\n",
    "    my purposes\n",
    "    http://sentiment.christopherpotts.net/index.html\n",
    "\"\"\"\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "regex_strings = (\n",
    "    # Emoticons:\n",
    "    emoticon_string,\n",
    "    # HTML tags:\n",
    "    r'<[^>]+>',\n",
    "    # Twitter username:\n",
    "    r'(?:@[\\w_]+)',\n",
    "    # Links\n",
    "    r'http\\S+',\n",
    "    # Twitter hashtags:\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "word_re = re.compile(r'(%s)' % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "emoticon_re = re.compile(regex_strings[0], re.VERBOSE | re.I | re.UNICODE)\n",
    "html_entity_digit_re = re.compile(r'&#\\d+;')\n",
    "html_entity_alpha_re = re.compile(r'&\\w+;')\n",
    "amp = \"&amp;\"\n",
    "url_re = re.compile(r'http\\S+')\n",
    "rep_char_re = re.compile(r'(\\w)\\1{3,}')\n",
    "hashtag_re = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "user_tag_re = re.compile(r'(?:@[\\w_]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeatureCombiner(object):\n",
    "\n",
    "    def transform(self, X, pre):\n",
    "        pre.normalise_vect()\n",
    "        feats = X\n",
    "        for k, v in pre.feats.iteritems():\n",
    "            feats = np.c_[feats, np.array(v)]\n",
    "        return feats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "def build_and_evaluate(n_gram, min_df, max_df, norm, clf, pos_tags):\n",
    "\n",
    "    def preprocess(s):\n",
    "        return preprocessor.tokenise(s, pos_tags)\n",
    "\n",
    "    X = train['text'].values\n",
    "    y = train['class'].values\n",
    "    X_test = test['text'].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    # Initialise transformers/estimators\n",
    "    preprocessor = Preprocessor()\n",
    "    feat_comb = FeatureCombiner()\n",
    "    vec = TfidfVectorizer(tokenizer=preprocess,\n",
    "                          lowercase=False,\n",
    "                          ngram_range=n_gram,\n",
    "                          min_df=min_df,\n",
    "                          max_df=max_df, \n",
    "                          norm=norm)  \n",
    "\n",
    "    # Build model\n",
    "    print(\"Building model\")\n",
    "    tfidf_matrix = vec.fit_transform(X)\n",
    "    clf.fit(tfidf_matrix, y)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    tfidf_matrix_test = vec.transform(X_test)\n",
    "    y_pred = clf.predict(tfidf_matrix_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print np.mean(y_pred == y_test)\n",
    "    print cm(y_test, y_pred)\n",
    "    print(clsr(y_test, y_pred, target_names=['obj', 'sub']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit with -\n",
    "- tfidf, unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.665467625899\n",
      "[[86 53]\n",
      " [40 99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.68      0.62      0.65       139\n",
      "        sub       0.65      0.71      0.68       139\n",
      "\n",
      "avg / total       0.67      0.67      0.66       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "n_gram=(1, 1)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit with -\n",
    "- tfidf, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.68345323741\n",
      "[[97 42]\n",
      " [46 93]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.68      0.70      0.69       139\n",
      "        sub       0.69      0.67      0.68       139\n",
      "\n",
      "avg / total       0.68      0.68      0.68       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit with -\n",
    "- tfidf, bigrams, pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.625899280576\n",
      "[[102  37]\n",
      " [ 67  72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.60      0.73      0.66       139\n",
      "        sub       0.66      0.52      0.58       139\n",
      "\n",
      "avg / total       0.63      0.63      0.62       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with -\n",
    "- tfidf, unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.679856115108\n",
      "[[ 86  53]\n",
      " [ 36 103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.70      0.62      0.66       139\n",
      "        sub       0.66      0.74      0.70       139\n",
      "\n",
      "avg / total       0.68      0.68      0.68       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.9)\n",
    "n_gram = (1, 1)\n",
    "model = build_and_evaluate(n_gram , 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with -\n",
    "- tfidf, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.697841726619\n",
      "[[98 41]\n",
      " [43 96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.70      0.71      0.70       139\n",
      "        sub       0.70      0.69      0.70       139\n",
      "\n",
      "avg / total       0.70      0.70      0.70       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.9)\n",
    "n_gram = (1, 2)\n",
    "model = build_and_evaluate(n_gram , 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with -\n",
    "- tfidf, bigrams, pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.651079136691\n",
      "[[95 44]\n",
      " [53 86]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.64      0.68      0.66       139\n",
      "        sub       0.66      0.62      0.64       139\n",
      "\n",
      "avg / total       0.65      0.65      0.65       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.9)\n",
    "n_gram = (1, 2)\n",
    "model = build_and_evaluate(n_gram , 1, 0.8, 'l2', clf, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifiers (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with -\n",
    "- tfidf, unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.604316546763\n",
      "[[ 61  78]\n",
      " [ 32 107]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.66      0.44      0.53       139\n",
      "        sub       0.58      0.77      0.66       139\n",
      "\n",
      "avg / total       0.62      0.60      0.59       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 1)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with -\n",
    "- tfidf, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.643884892086\n",
      "[[85 54]\n",
      " [45 94]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.65      0.61      0.63       139\n",
      "        sub       0.64      0.68      0.66       139\n",
      "\n",
      "avg / total       0.64      0.64      0.64       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with -\n",
    "- tfidf, bigrams, pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.589928057554\n",
      "[[92 47]\n",
      " [67 72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.58      0.66      0.62       139\n",
      "        sub       0.61      0.52      0.56       139\n",
      "\n",
      "avg / total       0.59      0.59      0.59       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
