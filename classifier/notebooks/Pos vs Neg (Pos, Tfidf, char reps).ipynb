{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of Pos/Neg tweets\n",
    "## Features:\n",
    "POS tags + tfidf count in BoW style\n",
    "Binary features for character repetitions in tweet\n",
    "Binary features for acronym presence in tweet\n",
    "Binary features for presence of happy or sad emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = p.read_csv('./data/train_raw.csv', usecols=(['class', 'text'])).dropna()\n",
    "test  = p.read_csv('./data/test_data1.csv', usecols=(['class', 'text'])).dropna()\n",
    "train = train.reindex(np.random.permutation(train.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor class\n",
    "Helper class which tokenises tweets and includes pos tags and creates binary features from character repitions, presence of acronyms in a tweet and seperate binary features for happy and sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "import re\n",
    "import htmlentitydefs\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class Preprocessor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopwords = list(sw.words('english'))\n",
    "        self.feats = {'reps': []}#, 'acry': []} #, 'happy': [], 'sad': []}\n",
    "        self.word_re = word_re\n",
    "        self.emoticon_re = emoticon_re\n",
    "        self.html_entity_digit_re = html_entity_digit_re\n",
    "        self.html_entity_alpha_re = html_entity_alpha_re\n",
    "        self.amp = amp\n",
    "        self.punct_re = punct_re\n",
    "        self.negation_re = negation_re\n",
    "        self.url_re = url_re\n",
    "        self.rep_char_re = rep_char_re\n",
    "        self.hashtag_re = hashtag_re\n",
    "        self.user_tag_re = user_tag_re\n",
    "        self.acrynoms = self.load_acrynoms()\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def load_acrynoms(self):\n",
    "        with open('./data/acrynom.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            slang = dict((rows[0], rows[1]) for rows in reader)\n",
    "            return slang\n",
    "        \n",
    "    def pos_tags(self, tokens):\n",
    "        TAG_MAP = [ \"NN\", \"NNP\", \"NNS\", \"VBP\", \"VB\", \"VBD\", 'VBG', \"VBN\",\n",
    "                    \"VBZ\", \"MD\",\"UH\", \"PRP\", \"PRP$\"]\n",
    "        tags = pos_tag(tokens)\n",
    "        return [tag[1] for tag in tags if tag[1] in TAG_MAP]\n",
    "\n",
    "    \n",
    "    def reset_feats(self):\n",
    "        self.feats = {k: [] for k, v in self.feats.iteritems()}\n",
    "\n",
    "    def normalise(self, tokens):\n",
    "        \n",
    "        vect = []\n",
    "        \n",
    "        for t in tokens:\n",
    "\n",
    "            if t in string.punctuation or t in self.stopwords:\n",
    "                continue\n",
    "                \n",
    "            if not self.emoticon_re.search(t):\n",
    "                t = t.lower()\n",
    "            \n",
    "            t = self.rep_char_re.sub(r'\\1', t)\n",
    "            t = self.url_re.sub('_URL', t)\n",
    "            t = self.hashtag_re.sub('_HASH', t)\n",
    "            t = self.user_tag_re.sub('_USER', t)\n",
    "            \n",
    "            vect.append(self.stemmer.stem(t))\n",
    "\n",
    "        tags = self.pos_tags(tokens)\n",
    "        vect = tags + vect\n",
    "        return vect\n",
    "    \n",
    "    def tokenise(self, tweet):\n",
    "        tweet = self.__html2unicode(tweet)\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "#         self.char_repititions(tokens)\n",
    "        return self.normalise(tokens)\n",
    "    \n",
    "    def tokenise_not_normalised(self, tweet):\n",
    "        tweet = self.__html2unicode(tweet)\n",
    "        return self.word_re.findall(tweet)\n",
    "    \n",
    "    def append_binary_feats(self, intensify, feat):\n",
    "        if intensify:\n",
    "            self.feats[feat].append([1])\n",
    "        else:\n",
    "            self.feats[feat].append([0])\n",
    "    \n",
    "    def char_repititions(self, tokens):\n",
    "#         reps = any(self.rep_char_re.search(word) for word in tokens)\n",
    "#         self.append_binary_feats(reps, 'reps')\n",
    "        return any(self.rep_char_re.search(word) for word in tokens)\n",
    "\n",
    "    def ensure_unicode(self, tweet):\n",
    "        try:\n",
    "            return unicode(tweet)\n",
    "        except UnicodeDecodeError:\n",
    "            tweet = str(tweet).encode('string_escape')\n",
    "            return unicode(tweet)\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        This function is curtosy of Christopher Potts\n",
    "        http://sentiment.christopherpotts.net/index.html\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(self.html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(self.html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x: x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:\n",
    "                s = s.replace(ent,\n",
    "                              unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass\n",
    "            s = s.replace(self.amp, \" and \")\n",
    "        return s\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "    This file is based on the work of Christopher Potts\n",
    "    however the file has been altered and extended for\n",
    "    my purposes\n",
    "    http://sentiment.christopherpotts.net/index.html\n",
    "\"\"\"\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?\n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?\n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*\n",
    "      \\d{4}          # base\n",
    "    )\"\"\",\n",
    "    # Emoticons:\n",
    "    emoticon_string,\n",
    "    # HTML tags:\n",
    "    r'<[^>]+>',\n",
    "    # Twitter username:\n",
    "    r'(?:@[\\w_]+)',\n",
    "    # Links\n",
    "    r'http\\S+',\n",
    "    # Twitter hashtags:\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                    # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "negation_words = (\n",
    "    \"\"\"\n",
    "    (?x)(?:\n",
    "    ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "        havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "     )$\n",
    "    )\n",
    "    |\n",
    "    n't\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# ######################################################################\n",
    "\n",
    "word_re = re.compile(r'(%s)' % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "html_entity_digit_re = re.compile(r'&#\\d+;')\n",
    "html_entity_alpha_re = re.compile(r'&\\w+;')\n",
    "amp = \"&amp;\"\n",
    "punct_re = re.compile(\"^[.:;!?]$\")\n",
    "negation_re = re.compile(negation_words)\n",
    "url_re = re.compile(r'http\\S+')\n",
    "rep_char_re = re.compile(r'(\\w)\\1{3,}')\n",
    "hashtag_re = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "user_tag_re = re.compile(r'(?:@[\\w_]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use character repetitions as binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "70638\n",
      "359\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "tokenisor = Preprocessor()\n",
    "\n",
    "def char_repitions(tweet):\n",
    "    tokens = tokenisor.tokenise_not_normalised(tweet.decode('utf-8'))\n",
    "    if tokenisor.char_repititions(tokens):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train['repetitions'] = train['text'].map(lambda x: char_repitions(x))\n",
    "test['repetitions'] = test['text'].map(lambda x: char_repitions(x)) \n",
    "print len(train)\n",
    "print train['repetitions'].sum()\n",
    "print len(test)\n",
    "print test['repetitions'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "class FeatureCombiner(object):\n",
    "\n",
    "    def transform(self, X, pre):\n",
    "#         pre.normalise_vect()\n",
    "        feats = X\n",
    "        for k, v in pre.feats.iteritems():\n",
    "            feats = np.c_[feats, np.array(v)]\n",
    "        return feats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "def build_and_evaluate(X, y, X_test, y_test, n_gram, min_df, max_df, norm, clf, outpath=None):\n",
    "\n",
    "    def preprocess(s):\n",
    "        return preprocessor.tokenise(s)\n",
    "\n",
    "    # Initialise transformers/estimators\n",
    "    preprocessor = Preprocessor()\n",
    "    vec = TfidfVectorizer(tokenizer=preprocess,\n",
    "                          lowercase=False,\n",
    "                          ngram_range=n_gram,\n",
    "                          min_df=min_df,\n",
    "                          max_df=max_df, \n",
    "                          norm=norm)\n",
    "#                           max_features=5000,\n",
    "\n",
    "    # Build model\n",
    "    print(\"Building model\")\n",
    "    tfidf_matrix = vec.fit_transform(X)\n",
    "    features_train = np.concatenate((tfidf_matrix.toarray(), train['repetitions'].as_matrix()), axis=1)\n",
    "    clf.fit(features_train, y)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    tfidf_matrix_test = vec.transform(X_test)\n",
    "    features_test = np.concatenate((tfidf_matrix_test.toarray(), test['repetitions'].as_matrix()), axis=1)\n",
    "    y_pred = clf.predict(features_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print np.mean(y_pred == y_test)\n",
    "    print cm(y_test, y_pred)\n",
    "    print(clsr(y_test, y_pred, target_names=['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 1)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values, \n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf = MultinomialNB()\n",
    "# n_gram = (1, 1)\n",
    "# model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "#                            test['text'].values, test['class'].values, \n",
    "#                            n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
