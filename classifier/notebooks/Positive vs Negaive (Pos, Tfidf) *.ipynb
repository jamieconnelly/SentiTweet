{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of Positive/Negative tweets\n",
    "## Features: \n",
    "- tfidf count\n",
    "- POS tags in BoW fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as p\n",
    "import numpy as np\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = p.read_csv('../data/train_raw.csv', usecols=(['class', 'text'])).dropna()\n",
    "test  = p.read_csv('../data/test_data1.csv', usecols=(['class', 'text'])).dropna()\n",
    "train = train.reindex(np.random.permutation(train.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor class\n",
    "Helper class which tokenises tweets and creates additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"Tweet tokenisor and feature extractor.\n",
    "\n",
    "       Attributes:\n",
    "           feats (dict of lists): Contains counts for lexicon features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stopwords = list(sw.words('english'))\n",
    "        self.word_re = word_re\n",
    "        self.emoticon_re = emoticon_re\n",
    "        self.url_re = url_re\n",
    "        self.rep_char_re = rep_char_re\n",
    "        self.hashtag_re = hashtag_re\n",
    "        self.user_tag_re = user_tag_re\n",
    "        self.lexicon = self._load_lexicon()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.feats = {'pos': [], 'neg': []}\n",
    "\n",
    "    def tokenise(self, tweet, pos_tags=False):\n",
    "        \"\"\"Tweet tokenisor method.\n",
    "\n",
    "            Args:\n",
    "                tweet (str): Text of a tweet.\n",
    "                lexicon_feats (bool): Whether to include lexicon\n",
    "                    features or not.\n",
    "\n",
    "            Returns:\n",
    "                Returns list of str.\n",
    "        \"\"\"\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "        return self._normalise(tokens, pos_tags)\n",
    "\n",
    "    def reset_feats(self):\n",
    "        \"\"\"Re-initialises the feats attribute.\"\"\"\n",
    "        self.feats = {k: [] for k, v in self.feats.iteritems()}\n",
    "\n",
    "    def normalise_vect(self):\n",
    "        \"\"\"Normalises each value in the feats attribute.\"\"\"\n",
    "        max_val = 0\n",
    "        for v in self.feats.itervalues():\n",
    "            temp = max(map(lambda x: x[0], v))\n",
    "            if temp > max_val:\n",
    "                max_val = temp\n",
    "\n",
    "        for k, v in self.feats.iteritems():\n",
    "            self.feats[k] = map(lambda x: [0] if x[0] == 0 else [(x[0] / max_val) * 15], v)\n",
    "\n",
    "    def _load_lexicon(self):\n",
    "        with open('../app/data/lexicon.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            return dict((rows[2], rows[5]) for rows in reader)\n",
    "\n",
    "    def _pos_tags(self, tokens):\n",
    "        TAG_MAP = [\"NN\", \"NNP\", \"NNS\", \"VBP\", \"VB\", \"VBD\", 'VBG',\n",
    "                   \"VBN\", \"VBZ\", \"MD\",\"UH\", \"PRP\", \"PRP$\"]\n",
    "        tags = pos_tag(tokens)\n",
    "        return [tag[1] for tag in tags if tag[1] in TAG_MAP]\n",
    "\n",
    "    def _normalise(self, tokens, pos_tags):\n",
    "\n",
    "        token_list = []\n",
    "\n",
    "        for t in tokens:\n",
    "            # Ignore stopwords\n",
    "            if t in self.stopwords:\n",
    "                continue\n",
    "\n",
    "            # lowercase all tokens except for emoticons\n",
    "            if not self.emoticon_re.search(t):\n",
    "                t = t.lower()\n",
    "\n",
    "            # Normalise tokens\n",
    "            t = self.rep_char_re.sub(r'\\1', t)\n",
    "            t = self.url_re.sub('_URL', t)\n",
    "            t = self.hashtag_re.sub('_HASH', t)\n",
    "            t = self.user_tag_re.sub('_USER', t)\n",
    "\n",
    "            # Get token's stem and append it to the list\n",
    "            token_list.append(self.stemmer.stem(t))\n",
    "\n",
    "        # Get list of pos tags and append to token_list\n",
    "        if pos_tags:\n",
    "            tags = self._pos_tags(tokens)\n",
    "            token_list = tags + token_list\n",
    "        return token_list\n",
    "\n",
    "    def _lexicon_lookup(self, tokens):\n",
    "        # Initialise new 'row' to list\n",
    "        for k, v in self.feats.iteritems():\n",
    "            self.feats[k].append([0])\n",
    "\n",
    "        idx = len(self.feats['pos']) - 1\n",
    "\n",
    "        # Check if token is in lexicon dictionary.\n",
    "        # If it is increment pos/neg feature count\n",
    "        for t in tokens:\n",
    "            t = t.lower()\n",
    "            if t in self.lexicon:\n",
    "                if self.lexicon[t] == '4':\n",
    "                    self.feats['pos'][idx][0] += 1\n",
    "                elif self.lexicon[t] == '0':\n",
    "                    self.feats['neg'][idx][0] += 1\n",
    "\n",
    "\"\"\"\n",
    "    This file is based on the work of Christopher Potts.\n",
    "    However, the file has been altered and extended for\n",
    "    my purposes\n",
    "    http://sentiment.christopherpotts.net/index.html\n",
    "\"\"\"\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "regex_strings = (\n",
    "    # Emoticons:\n",
    "    emoticon_string,\n",
    "    # HTML tags:\n",
    "    r'<[^>]+>',\n",
    "    # Twitter username:\n",
    "    r'(?:@[\\w_]+)',\n",
    "    # Links\n",
    "    r'http\\S+',\n",
    "    # Twitter hashtags:\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "word_re = re.compile(r'(%s)' % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "emoticon_re = re.compile(regex_strings[0], re.VERBOSE | re.I | re.UNICODE)\n",
    "html_entity_digit_re = re.compile(r'&#\\d+;')\n",
    "html_entity_alpha_re = re.compile(r'&\\w+;')\n",
    "amp = \"&amp;\"\n",
    "url_re = re.compile(r'http\\S+')\n",
    "rep_char_re = re.compile(r'(\\w)\\1{3,}')\n",
    "hashtag_re = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "user_tag_re = re.compile(r'(?:@[\\w_]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeatureCombiner(object):\n",
    "\n",
    "    def transform(self, X, pre):\n",
    "        pre.normalise_vect()\n",
    "        feats = X\n",
    "        for k, v in pre.feats.iteritems():\n",
    "            feats = np.c_[feats, np.array(v)]\n",
    "        return feats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "def build_and_evaluate(n_gram, min_df, max_df, norm, clf, pos_tags):\n",
    "\n",
    "    def preprocess(s):\n",
    "        return preprocessor.tokenise(s, pos_tags)\n",
    "\n",
    "    X = train['text'].values\n",
    "    y = train['class'].values\n",
    "    X_test = test['text'].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    # Initialise transformers/estimators\n",
    "    preprocessor = Preprocessor()\n",
    "    feat_comb = FeatureCombiner()\n",
    "    vec = TfidfVectorizer(tokenizer=preprocess,\n",
    "                          lowercase=False,\n",
    "                          ngram_range=n_gram,\n",
    "                          min_df=min_df,\n",
    "                          max_df=max_df, \n",
    "                          norm=norm)  \n",
    "\n",
    "    # Build model\n",
    "    print(\"Building model\")\n",
    "    tfidf_matrix = vec.fit_transform(X)\n",
    "    clf.fit(tfidf_matrix, y)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    tfidf_matrix_test = vec.transform(X_test)\n",
    "    y_pred = clf.predict(tfidf_matrix_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print np.mean(y_pred == y_test)\n",
    "    print cm(y_test, y_pred)\n",
    "    print(clsr(y_test, y_pred, target_names=['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit with -\n",
    "- tfidf, unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.799442896936\n",
      "[[131  46]\n",
      " [ 26 156]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.74      0.78       177\n",
      "        pos       0.77      0.86      0.81       182\n",
      "\n",
      "avg / total       0.80      0.80      0.80       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 1)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit with -\n",
    "- tfidf, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.8356545961\n",
      "[[145  32]\n",
      " [ 27 155]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.82      0.83       177\n",
      "        pos       0.83      0.85      0.84       182\n",
      "\n",
      "avg / total       0.84      0.84      0.84       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit with -\n",
    "- tfidf, bigrams, pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.846796657382\n",
      "[[145  32]\n",
      " [ 23 159]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.86      0.82      0.84       177\n",
      "        pos       0.83      0.87      0.85       182\n",
      "\n",
      "avg / total       0.85      0.85      0.85       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with -\n",
    "- tfidf, unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.802228412256\n",
      "[[138  39]\n",
      " [ 32 150]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.81      0.78      0.80       177\n",
      "        pos       0.79      0.82      0.81       182\n",
      "\n",
      "avg / total       0.80      0.80      0.80       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.9)\n",
    "n_gram = (1, 1)\n",
    "model = build_and_evaluate(n_gram , 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with -\n",
    "- tfidf, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.83286908078\n",
      "[[146  31]\n",
      " [ 29 153]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.82      0.83       177\n",
      "        pos       0.83      0.84      0.84       182\n",
      "\n",
      "avg / total       0.83      0.83      0.83       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.9)\n",
    "n_gram = (1, 2)\n",
    "model = build_and_evaluate(n_gram , 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with -\n",
    "- tfidf, bigrams, pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.827298050139\n",
      "[[143  34]\n",
      " [ 28 154]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.81      0.82       177\n",
      "        pos       0.82      0.85      0.83       182\n",
      "\n",
      "avg / total       0.83      0.83      0.83       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.9)\n",
    "n_gram = (1, 2)\n",
    "model = build_and_evaluate(n_gram , 1, 0.8, 'l2', clf, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifiers (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with -\n",
    "- tfidf, unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.802228412256\n",
      "[[131  46]\n",
      " [ 25 157]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.74      0.79       177\n",
      "        pos       0.77      0.86      0.82       182\n",
      "\n",
      "avg / total       0.81      0.80      0.80       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 1)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with -\n",
    "- tfidf, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.774373259053\n",
      "[[122  55]\n",
      " [ 26 156]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.82      0.69      0.75       177\n",
      "        pos       0.74      0.86      0.79       182\n",
      "\n",
      "avg / total       0.78      0.77      0.77       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with -\n",
    "- tfidf, bigrams, pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.75208913649\n",
      "[[117  60]\n",
      " [ 29 153]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.80      0.66      0.72       177\n",
      "        pos       0.72      0.84      0.77       182\n",
      "\n",
      "avg / total       0.76      0.75      0.75       359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(n_gram, 1, 0.8, 'l2', clf, True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
