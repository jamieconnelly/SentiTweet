{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of Subjective/Objective tweets\n",
    "## Features: \n",
    "- NER + POS tags + tfidf count in BoW style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5946 278\n"
     ]
    }
   ],
   "source": [
    "train = p.read_csv('./sub_obj_data/train_new.csv', usecols=(['class', 'text'])).dropna()\n",
    "train = train.reindex(np.random.permutation(train.index))\n",
    "train_large = p.read_csv('./sub_obj_data/training.csv', usecols=(['class', 'text'])).dropna()\n",
    "train_large = train_large.reindex(np.random.permutation(train_large.index))\n",
    "\n",
    "test  = p.read_csv('./sub_obj_data/test_ds.csv', usecols=(['class', 'text'])).dropna()\n",
    "\n",
    "print len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor class\n",
    "Helper class which tokenises tweets and includes pos tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "import re\n",
    "import htmlentitydefs\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class Preprocessor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopwords = list(sw.words('english'))\n",
    "        self.word_re = word_re\n",
    "        self.emoticon_re = emoticon_re\n",
    "        self.html_entity_digit_re = html_entity_digit_re\n",
    "        self.html_entity_alpha_re = html_entity_alpha_re\n",
    "        self.amp = amp\n",
    "        self.punct_re = punct_re\n",
    "        self.negation_re = negation_re\n",
    "        self.url_re = url_re\n",
    "        self.rep_char_re = rep_char_re\n",
    "        self.hashtag_re = hashtag_re\n",
    "        self.user_tag_re = user_tag_re\n",
    "        self.acrynoms = self.load_acrynoms()\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def load_acrynoms(self):\n",
    "        with open('./data/acrynom.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            slang = dict((rows[0], rows[1]) for rows in reader)\n",
    "            return slang\n",
    "        \n",
    "    def pos_tags(self, tokens):\n",
    "        TAG_MAP = [ \"NN\", \"NNP\", \"NNS\", \"VBP\", \"VB\", \"VBD\", 'VBG', \"VBN\",\n",
    "                    \"VBZ\", \"MD\",\"UH\", \"PRP\", \"PRP$\"]\n",
    "        tags = pos_tag(tokens)\n",
    "        return [tag[1] for tag in tags if tag[1] in TAG_MAP]\n",
    "\n",
    "    def normalise(self, tokens, ner):\n",
    "        \n",
    "        vect = ner\n",
    "        \n",
    "        for t in tokens:\n",
    "\n",
    "            if t in self.stopwords: #t in string.punctuation or t in self.stopwords:\n",
    "                continue\n",
    "                \n",
    "            if not self.emoticon_re.search(t):\n",
    "                t = t.lower()       \n",
    "            \n",
    "            t = self.rep_char_re.sub(r'\\1', t)\n",
    "            t = self.url_re.sub('_URL', t)\n",
    "            t = self.hashtag_re.sub('_HASH', t)\n",
    "            t = self.user_tag_re.sub('_USER', t)\n",
    "            \n",
    "            vect.append(self.stemmer.stem(t))\n",
    "\n",
    "        tags = self.pos_tags(tokens)\n",
    "        vect = tags + vect\n",
    "        return vect\n",
    "    \n",
    "    def tokenise(self, tweet):\n",
    "        tweet = self.__html2unicode(tweet)\n",
    "        nlp = spacy.load('en')\n",
    "        doc = nlp(tweet)\n",
    "        ner = []\n",
    "        for ent in doc.ents:\n",
    "            ner.append('_' + ent.label_)\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "        return self.normalise(tokens, ner)\n",
    "\n",
    "    def ensure_unicode(self, tweet):\n",
    "        try:\n",
    "            return unicode(tweet)\n",
    "        except UnicodeDecodeError:\n",
    "            tweet = str(tweet).encode('string_escape')\n",
    "            return unicode(tweet)\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        This function is curtosy of Christopher Potts\n",
    "        http://sentiment.christopherpotts.net/index.html\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(self.html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(self.html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x: x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:\n",
    "                s = s.replace(ent,\n",
    "                              unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass\n",
    "            s = s.replace(self.amp, \" and \")\n",
    "        return s\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "    This file is based on the work of Christopher Potts\n",
    "    however the file has been altered and extended for\n",
    "    my purposes\n",
    "    http://sentiment.christopherpotts.net/index.html\n",
    "\"\"\"\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?\n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?\n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*\n",
    "      \\d{4}          # base\n",
    "    )\"\"\",\n",
    "    # Emoticons:\n",
    "    emoticon_string,\n",
    "    # HTML tags:\n",
    "    r'<[^>]+>',\n",
    "    # Twitter username:\n",
    "    r'(?:@[\\w_]+)',\n",
    "    # Links\n",
    "    r'http\\S+',\n",
    "    # Twitter hashtags:\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                    # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "negation_words = (\n",
    "    \"\"\"\n",
    "    (?x)(?:\n",
    "    ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "        havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "     )$\n",
    "    )\n",
    "    |\n",
    "    n't\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# ######################################################################\n",
    "\n",
    "word_re = re.compile(r'(%s)' % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "html_entity_digit_re = re.compile(r'&#\\d+;')\n",
    "html_entity_alpha_re = re.compile(r'&\\w+;')\n",
    "amp = \"&amp;\"\n",
    "punct_re = re.compile(\"^[.:;!?]$\")\n",
    "negation_re = re.compile(negation_words)\n",
    "url_re = re.compile(r'http\\S+')\n",
    "rep_char_re = re.compile(r'(\\w)\\1{3,}')\n",
    "hashtag_re = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "user_tag_re = re.compile(r'(?:@[\\w_]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "def build_and_evaluate(X, y, X_test, y_test, n_gram, min_df, max_df, norm, clf, outpath=None):\n",
    "\n",
    "    def preprocess(s):\n",
    "        return preprocessor.tokenise(s)\n",
    "\n",
    "    # Initialise transformers/estimators\n",
    "    preprocessor = Preprocessor()\n",
    "    vec = TfidfVectorizer(tokenizer=preprocess,\n",
    "                          lowercase=False,\n",
    "                          ngram_range=n_gram,\n",
    "                          min_df=min_df,\n",
    "                          max_df=max_df, \n",
    "                          norm=norm)\n",
    "#                           max_features=5000)\n",
    "    \n",
    "\n",
    "    # Build model\n",
    "    print(\"Building model\")\n",
    "    tfidf_matrix = vec.fit_transform(X)\n",
    "    clf.fit(tfidf_matrix, y)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    tfidf_matrix_test = vec.transform(X_test)\n",
    "    y_pred = clf.predict(tfidf_matrix_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print np.mean(y_pred == y_test)\n",
    "    print cm(y_test, y_pred)\n",
    "    print(clsr(y_test, y_pred, target_names=['obj', 'sub']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 1)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "n_gram = (1, 1)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram , 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC(C=5)#, penalty='l1', dual=False)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression classifier large training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train_large['text'].values, train_large['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes large training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "n_gram = (1, 1)\n",
    "model = build_and_evaluate(train_large['text'].values, train_large['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram , 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC classifier large training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC(C=5)#, penalty='l1', dual=False)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train_large['text'].values, train_large['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier large training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train_large['text'].values, train_large['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'Jamie is happy today.')\n",
    "print doc.ents\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
