{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of Subjective/Objective tweets\n",
    "## Features: \n",
    "- POS tags + tfidf count in BoW style\n",
    "- Binary features for character repetitions in tweet\n",
    "- Binary features for acronym presence in tweet\n",
    "- Binary features for presence of happy or sad emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4162 1784\n"
     ]
    }
   ],
   "source": [
    "train = p.read_csv('./sub_obj_data/train_new.csv', usecols=(['class', 'text'])).dropna()\n",
    "test  = p.read_csv('./sub_obj_data/test_ds.csv', usecols=(['class', 'text'])).dropna()\n",
    "train = train.reindex(np.random.permutation(train.index))\n",
    "training_data, test_data, training_labels, test_labels = train_test_split(train['text'].values, train['class'].values, test_size=0.3, random_state=0) #70-30 split\n",
    "print len(training_data), len(test_data)\n",
    "# dev_data, eval_data, dev_labels, eval_labels = train_test_split(training_data, training_labels, test_size=0.5, random_state=0)\n",
    "# print len(dev_data), len(eval_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor class\n",
    "Helper class which tokenises tweets and includes pos tags and creates binary features from character repitions, presence of acronyms in a tweet and seperate binary features for happy and sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "import re\n",
    "import htmlentitydefs\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class Preprocessor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopwords = list(sw.words('english'))\n",
    "        self.feats = {'reps': [], 'acry': [], 'happy': [], 'sad': []}\n",
    "        self.emoji_happy = self.emoji_happy()\n",
    "        self.emoji_sad = self.emoji_sad()\n",
    "        self.emoji_dic = self.load_emoji_dict()\n",
    "        self.word_re = word_re\n",
    "        self.emoticon_re = emoticon_re\n",
    "        self.html_entity_digit_re = html_entity_digit_re\n",
    "        self.html_entity_alpha_re = html_entity_alpha_re\n",
    "        self.amp = amp\n",
    "        self.punct_re = punct_re\n",
    "        self.negation_re = negation_re\n",
    "        self.url_re = url_re\n",
    "        self.rep_char_re = rep_char_re\n",
    "        self.hashtag_re = hashtag_re\n",
    "        self.user_tag_re = user_tag_re\n",
    "        self.acrynoms = self.load_acrynoms()\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def load_acrynoms(self):\n",
    "        with open('./data/acrynom.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            slang = dict((rows[0], rows[1]) for rows in reader)\n",
    "            return slang\n",
    "        \n",
    "    def load_emoji_dict(self):\n",
    "        with open('./data/emoji_dict.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            return dict((rows[0], int(rows[1])) for rows in reader)\n",
    "        \n",
    "    def pos_tags(self, tokens):\n",
    "        TAG_MAP = [ \"NN\", \"NNP\", \"NNS\", \"VBP\", \"VB\", \"VBD\", 'VBG', \"VBN\",\n",
    "                    \"VBZ\", \"MD\",\"UH\", \"PRP\", \"PRP$\"]\n",
    "        tags = pos_tag(tokens)\n",
    "        return [tag[1] for tag in tags if tag[1] in TAG_MAP]\n",
    "\n",
    "    \n",
    "    def reset_feats(self):\n",
    "        self.feats = {k: [] for k, v in self.feats.iteritems()}\n",
    "\n",
    "    def normalise(self, tokens):\n",
    "        \n",
    "        vect = []\n",
    "        \n",
    "        for t in tokens:\n",
    "\n",
    "            if t in string.punctuation or t in self.stopwords:\n",
    "                continue\n",
    "                \n",
    "            if not self.emoticon_re.search(t):\n",
    "                t = t.lower()       \n",
    "            \n",
    "#             if t in self.acrynoms:\n",
    "#                 b = t \n",
    "#                 t = self.acrynoms[b]\n",
    "#             if self.rep_char_re.search(t):\n",
    "#                 vect.append('_REPS')\n",
    "            \n",
    "            t = self.rep_char_re.sub(r'\\1', t)\n",
    "            t = self.url_re.sub('_URL', t)\n",
    "            t = self.hashtag_re.sub('_HASH', t)\n",
    "            t = self.user_tag_re.sub('_USER', t)\n",
    "            \n",
    "            vect.append(self.stemmer.stem(t))\n",
    "\n",
    "        tags = self.pos_tags(tokens)\n",
    "        vect = tags + vect\n",
    "        return vect\n",
    "    \n",
    "    def tokenise(self, tweet):\n",
    "        tweet = self.__html2unicode(tweet)\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "#         self.caps_intensifier(tokens)\n",
    "        self.aryonms(tokens)\n",
    "        self.happy(tokens)\n",
    "        self.sad(tokens)\n",
    "        self.char_repititions(tokens)\n",
    "        return self.normalise(tokens)\n",
    "    \n",
    "    def append_binary_feats(self, intensify, feat):\n",
    "        if intensify:\n",
    "            self.feats[feat].append([1])\n",
    "        else:\n",
    "            self.feats[feat].append([0])\n",
    "    \n",
    "    def happy(self, tokens):\n",
    "        happy = any(self.emoji_happy.search(word) for word in tokens)\n",
    "        self.append_binary_feats(happy, 'happy')\n",
    "    \n",
    "    def sad(self, tokens):\n",
    "        sad = any(self.emoji_sad.search(word) for word in tokens)\n",
    "        self.append_binary_feats(sad, 'sad')\n",
    "    \n",
    "    def aryonms(self, tokens):\n",
    "        acrs = any(word in self.acrynoms for word in tokens)\n",
    "        self.append_binary_feats(acrs, 'acry')\n",
    "        \n",
    "    def char_repititions(self, tokens):\n",
    "        reps = any(self.rep_char_re.search(word) for word in tokens)\n",
    "        self.append_binary_feats(reps, 'reps')\n",
    "\n",
    "    def caps_intensifier(self, tokens):\n",
    "        caps = any(self.word_has_all_caps(word) for word in tokens)\n",
    "        self.append_binary_feats(caps, 'caps')\n",
    "\n",
    "    def word_has_all_caps(self, token):\n",
    "        if (self.emoticon_re.search(token)\n",
    "            or self.punct_re.match(token)\n",
    "                or self.has_num(token)):\n",
    "            return False\n",
    "\n",
    "        if (token.upper() == token\n",
    "            and (token != 'I'\n",
    "                 and token != 'A')):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def has_num(self, s):\n",
    "        return any(i.isdigit() for i in s)\n",
    "    \n",
    "    def emoji_happy(self):\n",
    "        try:\n",
    "            return re.compile(u'['\n",
    "                              u'\\U0001f600-\\U0001F60F'\n",
    "                              u'\\U0001F617-\\U0001F61D'\n",
    "                              u'\\U0001F638-\\U0001F63D'\n",
    "                              ']+', re.UNICODE)\n",
    "        except re.error:\n",
    "            return re.compile(u'('\n",
    "                              u'\\ud83d[\\ude00-\\ude0f]|'\n",
    "                              u'\\ud83d[\\ude17-\\ude1d]|'\n",
    "                              u'\\ud83d[\\ude38-\\ude3d]'\n",
    "                              ')+', re.UNICODE)\n",
    "\n",
    "    def emoji_sad(self):\n",
    "        try:\n",
    "            return re.compile(u'['\n",
    "                              u'\\U0001F612-\\U0001F616'\n",
    "                              u'\\U0001F61E-\\U0001F62B'\n",
    "                              u'\\U0001F63E-\\U0001F63F'\n",
    "                              ']+', re.UNICODE)\n",
    "        except re.error:\n",
    "            return re.compile(u'('\n",
    "                              u'\\ud83d[\\ude12-\\ude16]|'\n",
    "                              u'\\ud83d[\\ude1e-\\ude2b]|'\n",
    "                              u'\\ud83d[\\ude3e-\\ude3f]'\n",
    "                              ')+', re.UNICODE)\n",
    "\n",
    "    def ensure_unicode(self, tweet):\n",
    "        try:\n",
    "            return unicode(tweet)\n",
    "        except UnicodeDecodeError:\n",
    "            tweet = str(tweet).encode('string_escape')\n",
    "            return unicode(tweet)\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        This function is curtosy of Christopher Potts\n",
    "        http://sentiment.christopherpotts.net/index.html\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(self.html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(self.html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x: x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:\n",
    "                s = s.replace(ent,\n",
    "                              unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass\n",
    "            s = s.replace(self.amp, \" and \")\n",
    "        return s\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "    This file is based on the work of Christopher Potts\n",
    "    however the file has been altered and extended for\n",
    "    my purposes\n",
    "    http://sentiment.christopherpotts.net/index.html\n",
    "\"\"\"\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?\n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?\n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*\n",
    "      \\d{4}          # base\n",
    "    )\"\"\",\n",
    "    # Emoticons:\n",
    "    emoticon_string,\n",
    "    # HTML tags:\n",
    "    r'<[^>]+>',\n",
    "    # Twitter username:\n",
    "    r'(?:@[\\w_]+)',\n",
    "    # Links\n",
    "    r'http\\S+',\n",
    "    # Twitter hashtags:\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                    # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "negation_words = (\n",
    "    \"\"\"\n",
    "    (?x)(?:\n",
    "    ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "        havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "     )$\n",
    "    )\n",
    "    |\n",
    "    n't\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# ######################################################################\n",
    "\n",
    "word_re = re.compile(r'(%s)' % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "html_entity_digit_re = re.compile(r'&#\\d+;')\n",
    "html_entity_alpha_re = re.compile(r'&\\w+;')\n",
    "amp = \"&amp;\"\n",
    "punct_re = re.compile(\"^[.:;!?]$\")\n",
    "negation_re = re.compile(negation_words)\n",
    "url_re = re.compile(r'http\\S+')\n",
    "rep_char_re = re.compile(r'(\\w)\\1{3,}')\n",
    "hashtag_re = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "user_tag_re = re.compile(r'(?:@[\\w_]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "class FeatureCombiner(object):\n",
    "\n",
    "    def transform(self, X, pre):\n",
    "#         pre.normalise_vect()\n",
    "        feats = X\n",
    "        for k, v in pre.feats.iteritems():\n",
    "            feats = np.c_[feats, np.array(v)]\n",
    "        return feats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "def build_and_evaluate(X, y, X_test, y_test, n_gram, min_df, max_df, norm, clf, outpath=None):\n",
    "\n",
    "    def preprocess(s):\n",
    "        return preprocessor.tokenise(s)\n",
    "\n",
    "    # Initialise transformers/estimators\n",
    "    preprocessor = Preprocessor()\n",
    "    feat_comb = FeatureCombiner()\n",
    "    vec = TfidfVectorizer(tokenizer=preprocess,\n",
    "                          lowercase=False,\n",
    "                          ngram_range=n_gram,\n",
    "                          min_df=min_df,\n",
    "                          max_df=max_df, \n",
    "                          norm=norm)\n",
    "#                           max_features=5000,\n",
    "#                           ngram_range=(1, 3))\n",
    "    \n",
    "\n",
    "    # Build model\n",
    "    print(\"Building model\")\n",
    "    tfidf_matrix = vec.fit_transform(X)\n",
    "#     feat_matrix = feat_comb.transform(tfidf_matrix.todense(),\n",
    "#                                       preprocessor)\n",
    "    clf.fit(tfidf_matrix, y)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    preprocessor.reset_feats()\n",
    "    tfidf_matrix_test = vec.transform(X_test)\n",
    "#     feat_matrix_test = feat_comb.transform(tfidf_matrix_test.todense(),\n",
    "#                                            preprocessor)\n",
    "    y_pred = clf.predict(tfidf_matrix_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print np.mean(y_pred == y_test)\n",
    "    print cm(y_test, y_pred)\n",
    "    print(clsr(y_test, y_pred, target_names=['obj', 'sub']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.914798206278\n",
      "[[847  37]\n",
      " [115 785]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.88      0.96      0.92       884\n",
      "        sub       0.95      0.87      0.91       900\n",
      "\n",
      "avg / total       0.92      0.91      0.91      1784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(training_data, training_labels, test_data, test_labels, n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.913677130045\n",
      "[[802  82]\n",
      " [ 72 828]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.92      0.91      0.91       884\n",
      "        sub       0.91      0.92      0.91       900\n",
      "\n",
      "avg / total       0.91      0.91      0.91      1784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "n_gram = (1, 1)\n",
    "model = build_and_evaluate(training_data, training_labels, test_data, test_labels, n_gram , 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC(C=5)#, penalty='l1', dual=False)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(training_data, training_labels, test_data, test_labels, n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(training_data, training_labels, test_data, test_labels, n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with seperate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.604316546763\n",
      "[[79 60]\n",
      " [50 89]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.61      0.57      0.59       139\n",
      "        sub       0.60      0.64      0.62       139\n",
      "\n",
      "avg / total       0.60      0.60      0.60       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 1)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "slang = {}\n",
    "with open('./data/emoji_dict.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    slang = dict((rows[0], int(rows[1])) for rows in reader)\n",
    "    \n",
    "print slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove punct from pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to = Preprocessor()\n",
    "\n",
    "tweet = '@test HA helllllo i is ! i\\'ve his playing biggestttt'\n",
    "\n",
    "t = rep_char_re.match(tweet)\n",
    "t = to.tokenise(tweet.decode('utf-8'))\n",
    "print t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_and_evaluate(train['text'].values, train['class'].values, test['text'].values, test['class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_and_evaluate(train['text'].values, train['class'].values, test['text'].values, test['class'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
