{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of Subjective/Objective tweets\n",
    "## Features: \n",
    "- POS tags + tfidf count in BoW style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as p\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5946 278\n"
     ]
    }
   ],
   "source": [
    "train = p.read_csv('./sub_obj_data/train_new.csv', usecols=(['class', 'text'])).dropna()\n",
    "train = train.reindex(np.random.permutation(train.index))\n",
    "train_large = p.read_csv('./sub_obj_data/training.csv', usecols=(['class', 'text'])).dropna()\n",
    "train_large = train_large.reindex(np.random.permutation(train_large.index))\n",
    "\n",
    "test  = p.read_csv('./sub_obj_data/test_ds.csv', usecols=(['class', 'text'])).dropna()\n",
    "\n",
    "print len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor class\n",
    "Helper class which tokenises tweets and includes pos tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "import re\n",
    "import htmlentitydefs\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class Preprocessor():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopwords = list(sw.words('english'))\n",
    "        self.lexicon = self.load_lexicon()\n",
    "        self.lexicon_feats = {'pos': [], 'neg': [], 'neut': []}\n",
    "        self.word_re = word_re\n",
    "        self.emoticon_re = emoticon_re\n",
    "        self.html_entity_digit_re = html_entity_digit_re\n",
    "        self.html_entity_alpha_re = html_entity_alpha_re\n",
    "        self.amp = amp\n",
    "        self.punct_re = punct_re\n",
    "        self.negation_re = negation_re\n",
    "        self.url_re = url_re\n",
    "        self.rep_char_re = rep_char_re\n",
    "        self.hashtag_re = hashtag_re\n",
    "        self.user_tag_re = user_tag_re\n",
    "        self.acrynoms = self.load_acrynoms()\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def load_acrynoms(self):\n",
    "        with open('./data/acrynom.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            slang = dict((rows[0], rows[1]) for rows in reader)\n",
    "            return slang\n",
    "    \n",
    "    def load_lexicon(self):\n",
    "        with open('./data/lexicon.csv', 'rb') as f:\n",
    "            reader = csv.reader(f)\n",
    "            return dict((rows[2], rows[5]) for rows in reader)\n",
    "        \n",
    "    def pos_tags(self, tokens):\n",
    "        TAG_MAP = [ \"NN\", \"NNP\", \"NNS\", \"VBP\", \"VB\", \"VBD\", 'VBG', \"VBN\",\n",
    "                    \"VBZ\", \"MD\",\"UH\", \"PRP\", \"PRP$\"]\n",
    "        tags = pos_tag(tokens)\n",
    "        return [tag[1] for tag in tags if tag[1] in TAG_MAP]\n",
    "\n",
    "    def reset_feats(self):\n",
    "        self.lexicon_feats = {k: [] for k, v in self.lexicon_feats.iteritems()}\n",
    "        \n",
    "    def normalise_vect(self):\n",
    "        max_val = 0\n",
    "        for v in self.lexicon_feats.itervalues():\n",
    "            temp = max(map(lambda x: x[0], v))\n",
    "            if temp > max_val:\n",
    "                max_val = temp\n",
    "\n",
    "        for k, v in self.lexicon_feats.iteritems():\n",
    "            self.lexicon_feats[k] = map(lambda x: [0] if x[0] == 0 else [x[0]/max_val], v)\n",
    "    \n",
    "    def normalise(self, tokens):\n",
    "        \n",
    "        vect = []\n",
    "        \n",
    "        for t in tokens:\n",
    "\n",
    "            if t in string.punctuation or t in self.stopwords:\n",
    "                continue\n",
    "                \n",
    "            if not self.emoticon_re.search(t):\n",
    "                t = t.lower()\n",
    "            \n",
    "            if t in self.acrynoms:\n",
    "                t = self.acrynoms[t]\n",
    "            \n",
    "            t = self.rep_char_re.sub(r'\\1', t)\n",
    "            t = self.url_re.sub('_URL', t)\n",
    "            t = self.hashtag_re.sub('_HASH', t)\n",
    "            t = self.user_tag_re.sub('_USER', t)\n",
    "            \n",
    "            vect.append(self.stemmer.stem(t))\n",
    "\n",
    "        tags = self.pos_tags(tokens)\n",
    "        vect = tags + vect\n",
    "        return vect\n",
    "    \n",
    "    def lexicon_lookup(self, tokens):\n",
    "        polarities = ['pos', 'neg', 'neut']\n",
    "        for polarity in polarities:\n",
    "            self.lexicon_feats[polarity].append([0])\n",
    "        \n",
    "        idx = len(self.lexicon_feats['pos']) - 1\n",
    "        \n",
    "        for token in tokens:    \n",
    "            token = token.lower()\n",
    "            if token in self.lexicon:\n",
    "                polarity = self.lexicon[token]\n",
    "                if polarity == '4':\n",
    "                    self.lexicon_feats['pos'][idx][0] += 1\n",
    "                elif polarity == '0':\n",
    "                    self.lexicon_feats['neg'][idx][0] += 1\n",
    "                elif polarity == '2': \n",
    "                    self.lexicon_feats['neut'][idx][0] += 1\n",
    "#             else:\n",
    "#                 print 'not in lexicon'\n",
    "                \n",
    "#     def append_lexicon_feats(self, present, polarity):\n",
    "#         polarities = ['pos', 'neg', 'neut']\n",
    "#         for polarity in polarities:\n",
    "#             self.feats[polarity].append([0])\n",
    "        \n",
    "#         idx = len(self.feats['pos']) - 1\n",
    "#         if present:\n",
    "#             self.feats[polarity].append([1])\n",
    "#         else:\n",
    "#             self.feats[polarity].append([0])\n",
    "    \n",
    "    def tokenise(self, tweet):\n",
    "        tweet = self.__html2unicode(tweet)\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "#         self.lexicon_lookup(tokens)\n",
    "        return self.normalise(tokens)\n",
    "    \n",
    "    def tokenise_raw(self, tweet):\n",
    "        tweet = self.__html2unicode(tweet)\n",
    "        tokens = self.word_re.findall(tweet)\n",
    "\n",
    "    def ensure_unicode(self, tweet):\n",
    "        try:\n",
    "            return unicode(tweet)\n",
    "        except UnicodeDecodeError:\n",
    "            tweet = str(tweet).encode('string_escape')\n",
    "            return unicode(tweet)\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        This function is curtosy of Christopher Potts\n",
    "        http://sentiment.christopherpotts.net/index.html\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(self.html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(self.html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x: x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:\n",
    "                s = s.replace(ent,\n",
    "                              unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass\n",
    "            s = s.replace(self.amp, \" and \")\n",
    "        return s\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "    This file is based on the work of Christopher Potts\n",
    "    however the file has been altered and extended for\n",
    "    my purposes\n",
    "    http://sentiment.christopherpotts.net/index.html\n",
    "\"\"\"\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?\n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?\n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*\n",
    "      \\d{4}          # base\n",
    "    )\"\"\",\n",
    "    # Emoticons:\n",
    "    emoticon_string,\n",
    "    # HTML tags:\n",
    "    r'<[^>]+>',\n",
    "    # Twitter username:\n",
    "    r'(?:@[\\w_]+)',\n",
    "    # Links\n",
    "    r'http\\S+',\n",
    "    # Twitter hashtags:\n",
    "    r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)',\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                    # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "negation_words = (\n",
    "    \"\"\"\n",
    "    (?x)(?:\n",
    "    ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "        havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "        wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "     )$\n",
    "    )\n",
    "    |\n",
    "    n't\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# ######################################################################\n",
    "\n",
    "word_re = re.compile(r'(%s)' % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "html_entity_digit_re = re.compile(r'&#\\d+;')\n",
    "html_entity_alpha_re = re.compile(r'&\\w+;')\n",
    "amp = \"&amp;\"\n",
    "punct_re = re.compile(\"^[.:;!?]$\")\n",
    "negation_re = re.compile(negation_words)\n",
    "url_re = re.compile(r'http\\S+')\n",
    "rep_char_re = re.compile(r'(\\w)\\1{3,}')\n",
    "hashtag_re = re.compile(r'(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "user_tag_re = re.compile(r'(?:@[\\w_]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeatureCombiner(object):\n",
    "\n",
    "    def transform(self, X, pre):\n",
    "        pre.normalise_vect()\n",
    "        feats = X\n",
    "        for k, v in pre.lexicon_feats.iteritems():\n",
    "            feats = np.c_[feats, np.array(v)]\n",
    "        return feats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "def build_and_evaluate(X, y, X_test, y_test, n_gram, min_df, max_df, norm, clf, outpath=None):\n",
    "\n",
    "    def preprocess(s):\n",
    "        return preprocessor.tokenise(s)\n",
    "\n",
    "    # Initialise transformers/estimators\n",
    "    preprocessor = Preprocessor()\n",
    "    feat_comb = FeatureCombiner()\n",
    "    vec = TfidfVectorizer(tokenizer=preprocess,\n",
    "                          lowercase=False,\n",
    "                          ngram_range=n_gram,\n",
    "                          min_df=min_df,\n",
    "                          max_df=max_df, \n",
    "                          norm=norm)\n",
    "#                           max_features=5000)\n",
    "    \n",
    "\n",
    "    # Build model\n",
    "    print(\"Building model\")\n",
    "    tfidf_matrix = vec.fit_transform(X)\n",
    "#     feat_matrix = feat_comb.transform(tfidf_matrix.todense(),\n",
    "#                                       preprocessor)\n",
    "    clf.fit(tfidf_matrix, y)\n",
    "\n",
    "    # Evaluate on test set\n",
    "#     preprocessor.reset_feats()\n",
    "    tfidf_matrix_test = vec.transform(X_test)\n",
    "#     feat_matrix_test = feat_comb.transform(tfidf_matrix_test.todense(),\n",
    "#                                       preprocessor)\n",
    "    y_pred = clf.predict(tfidf_matrix_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print np.mean(y_pred == y_test)\n",
    "    print cm(y_test, y_pred)\n",
    "    print(clsr(y_test, y_pred, target_names=['obj', 'sub']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'neut': [0], u'neg': [1], u'pos': [3]}\n"
     ]
    }
   ],
   "source": [
    "# lexicon = p.read_csv('./data/lexicon.csv').dropna()\n",
    "# neg = lexicon[lexicon['polarity'] == 0].count()\n",
    "# neu = lexicon[lexicon['polarity'] == 2].count()\n",
    "# pos = lexicon[lexicon['polarity'] == 4].count()\n",
    "\n",
    "# def lexicon_lookup(tokens):\n",
    "    \n",
    "#     for token in tokens:    \n",
    "#         contains = lexicon['polarity'].loc[lexicon['word'] == token]\n",
    "#         if contains.values == [4]:\n",
    "#             print 'pos'\n",
    "#         elif contains.values == [0]:\n",
    "#             print 'neg'\n",
    "#         elif contains.values == [2]: \n",
    "#             print 'neut'\n",
    "#         else:\n",
    "#             print 'not in lexicon'\n",
    "            \n",
    "\n",
    "tokenisor = Preprocessor()\n",
    "# tokenisor.lexicon\n",
    "# print tokenisor.lexicon['happy']\n",
    "# if 'happy' in tokenisor.lexicon:\n",
    "#     polarity = tokenisor.lexicon['happy']\n",
    "#     print polarity\n",
    "# lexicon_lookup(['happy'])\n",
    "tokenisor.lexicon_lookup(['happy', 'happy', 'abandon', 'table', 'seperate', 'excited'])\n",
    "print tokenisor.lexicon_feats\n",
    "# tokenisor.lexicon_lookup(['happy', 'happy', 'abandon', 'table', 'seperate', 'excited'])\n",
    "# print tokenisor.lexicon_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.636690647482\n",
      "[[87 52]\n",
      " [49 90]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.64      0.63      0.63       139\n",
      "        sub       0.63      0.65      0.64       139\n",
      "\n",
      "avg / total       0.64      0.64      0.64       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "n_gram=(1, 1)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.687050359712\n",
      "[[ 86  53]\n",
      " [ 34 105]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.72      0.62      0.66       139\n",
      "        sub       0.66      0.76      0.71       139\n",
      "\n",
      "avg / total       0.69      0.69      0.69       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "n_gram = (1, 1)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram , 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.568345323741\n",
      "[[92 47]\n",
      " [73 66]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.56      0.66      0.61       139\n",
      "        sub       0.58      0.47      0.52       139\n",
      "\n",
      "avg / total       0.57      0.57      0.56       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(C=5)#, penalty='l1', dual=False)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.589928057554\n",
      "[[103  36]\n",
      " [ 78  61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.57      0.74      0.64       139\n",
      "        sub       0.63      0.44      0.52       139\n",
      "\n",
      "avg / total       0.60      0.59      0.58       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train['text'].values, train['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression classifier large training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=7)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train_large['text'].values, train_large['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes large training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n",
      "Classification Report:\n",
      "\n",
      "0.615107913669\n",
      "[[ 54  85]\n",
      " [ 22 117]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        obj       0.71      0.39      0.50       139\n",
      "        sub       0.58      0.84      0.69       139\n",
      "\n",
      "avg / total       0.64      0.62      0.59       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "n_gram = (1, 1)\n",
    "model = build_and_evaluate(train_large['text'].values, train_large['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram , 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC classifier large training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC(C=5)#, penalty='l1', dual=False)\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train_large['text'].values, train_large['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier large training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier()\n",
    "n_gram=(1, 2)\n",
    "model = build_and_evaluate(train_large['text'].values, train_large['class'].values,\n",
    "                           test['text'].values, test['class'].values,\n",
    "                           n_gram, 1, 0.8, 'l2', clf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
